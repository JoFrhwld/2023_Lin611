{
  "hash": "91ec136a8c15388f2a51f78ceea89cf7",
  "result": {
    "markdown": "---\ntitle: \"Continuing with the tidyverse\"\ndate: 2023-01-26\nresources: \n  - data/demographics.csv\norder: 07  \n---\n\n\n## Joins\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\nA useful diagram for understanding joins <https://r4ds.hadley.nz/joins.html#fig-join-left>\n\n::: callout-note\n## Work it out\n\n\n::: {.cell}\n\n```{.r .cell-code}\num <- read_tsv(\"https://bit.ly/3JdeSbx\")\ndemo <- read_csv(\"https://bit.ly/3wOfcGx\")\n```\n:::\n\n\nJoin these two datasets together (`dplyr::left_join()`) and find out\n\n1.  Which filled pause did people born before 1930 use the most?\n\n2.  Which filled pause did people born after 1980 use the most?\n\nThink of using functions like\n\n-   `dplyr::filter()`\n\n-   `dplyr::count()`\n\n-   `dplyr::arrange()`\n:::\n\n## Pivots\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"nasapower\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nasapower)\n```\n:::\n\n\n### Getting monthly temperature data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlex_temp <- \n  get_power(\n    community = \"ag\",\n    temporal_api = \"monthly\",\n    pars = \"T2M\",\n    dates = c(\"1985-01-01\", \"2021-12-31\"),\n    lonlat = c(-84.501640,  38.047989)\n  )\n```\n:::\n\n\nHere is monthly temperature data for Lexington according to NASA\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlex_temp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNASA/POWER CERES/MERRA2 Native Resolution Monthly and Annual  \n Dates (month/day/year): 01/01/1985 through 12/31/2021  \n Location: Latitude  38.048   Longitude -84.5016  \n Elevation from MERRA-2: Average for 0.5 x 0.625 degree lat/lon region = 280.65 meters \n The value for missing source data that cannot be computed or is outside of the sources availability range: NA  \n Parameter(s):  \n \n Parameters: \n T2M     MERRA-2 Temperature at 2 Meters (C)  \n \n# A tibble: 37 × 17\n     LON   LAT PARAMETER  YEAR   JAN   FEB   MAR   APR   MAY   JUN   JUL   AUG\n   <dbl> <dbl> <chr>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 -84.5  38.0 T2M        1985 -4.87 -3.06  7.64  13.5  17.4  21.0  23.8  23.1\n 2 -84.5  38.0 T2M        1986 -1.52  1.77  6.53  12.8  18.1  23.4  26.5  23.4\n 3 -84.5  38.0 T2M        1987 -0.99  1.08  6.54  10.8  19.7  22.8  25.4  25.7\n 4 -84.5  38.0 T2M        1988 -2.38 -0.56  5.58  11.0  17.2  23.6  26.4  26.2\n 5 -84.5  38.0 T2M        1989  2.42 -0.93  6.44  10.7  14.7  21.1  23.5  23.1\n 6 -84.5  38.0 T2M        1990  2.61  4.76  7.74  10.8  15.8  21.3  23.7  23.8\n 7 -84.5  38.0 T2M        1991 -0.54  2.13  7.15  13.5  20.4  22.3  24.3  24.3\n 8 -84.5  38.0 T2M        1992  0.09  3.51  6.23  11.7  15.9  19.8  23.6  20.9\n 9 -84.5  38.0 T2M        1993  1.78 -0.27  4.3   10.6  17.4  21.8  26.3  24.8\n10 -84.5  38.0 T2M        1994 -3.89  0.8   4.54  13.2  15.1  22.7  24.3  23.2\n# … with 27 more rows, and 5 more variables: SEP <dbl>, OCT <dbl>, NOV <dbl>,\n#   DEC <dbl>, ANN <dbl>\n```\n:::\n:::\n\n\n### Pivoting long\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlex_long <- \n  lex_temp |> \n  pivot_longer(\n    cols = JAN:DEC,\n    names_to = \"month\",\n    values_to = \"temp\"\n  )\n\nlex_long\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 444 × 7\n     LON   LAT PARAMETER  YEAR   ANN month  temp\n   <dbl> <dbl> <chr>     <dbl> <dbl> <chr> <dbl>\n 1 -84.5  38.0 T2M        1985  12.0 JAN   -4.87\n 2 -84.5  38.0 T2M        1985  12.0 FEB   -3.06\n 3 -84.5  38.0 T2M        1985  12.0 MAR    7.64\n 4 -84.5  38.0 T2M        1985  12.0 APR   13.5 \n 5 -84.5  38.0 T2M        1985  12.0 MAY   17.4 \n 6 -84.5  38.0 T2M        1985  12.0 JUN   21.0 \n 7 -84.5  38.0 T2M        1985  12.0 JUL   23.8 \n 8 -84.5  38.0 T2M        1985  12.0 AUG   23.1 \n 9 -84.5  38.0 T2M        1985  12.0 SEP   19.8 \n10 -84.5  38.0 T2M        1985  12.0 OCT   15.5 \n# … with 434 more rows\n```\n:::\n:::\n\n\n### Pivoting wide\n\nYou can do data operations, and then pivot the data back to wide.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlex_long |> \n  # converting year to a decade value\n  mutate(decade = floor(YEAR / 10)) |> \n  # grouping by decade and month\n  group_by(decade, month) |> \n  # getting average temperature within groups\n  summarise(mean_temp = mean(temp)) |> \n  # pivoting wide\n  pivot_wider(\n    names_from = month,\n    values_from = mean_temp\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'decade'. You can override using the\n`.groups` argument.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 13\n# Groups:   decade [5]\n  decade   APR   AUG    DEC   FEB    JAN   JUL   JUN   MAR   MAY   NOV   OCT\n   <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1    198  11.8  24.3 -0.564 -0.34 -1.47   25.1  22.4  6.55  17.4  7.50  12.4\n2    199  11.6  23.7  1.98   2.14  0.077  24.4  21.8  5.72  17.2  6.16  13.1\n3    200  12.7  24.5  1.08   1.23  0.158  24.1  22.3  6.86  17.7  7.34  13.6\n4    201  12.9  24.1  2.46   1.47 -0.838  24.7  22.7  6.26  18.6  6.08  14.0\n5    202  11.2  24.2  4.20   0.68  1.48   24.8  22.2  9.12  16.2  6.88  15.0\n# … with 1 more variable: SEP <dbl>\n```\n:::\n:::\n\n\n::: callout-note\n## Work it out\n\nWith the \"UM\" data\n\n1.  Calculate the average duration of the vowel for each person for each word. Think of using functions like\n    -   `dplyr::mutate()`\n\n    -   `dplyr::group_by()`\n\n    -   `dplyr::summarise()`\n2.  Figure out *how many times longer* the vowel is in \"UM\" than for \"UH\" for each person. Think of using functions like\n    -   `tidyr::pivot_wider()`\n\n    -   `dplyr::mutate()`\n:::\n\n## Untidy data examples\n\nSource: `{untidydata}` by Joseph Casillas\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"devtools\")\ndevtools::install_github(\"jvcasillas/untidydata\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(untidydata)\n```\n:::\n\n\nThis is just the `nettle` dataset all over again, but starts out \"long\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlanguage_diversity\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 444 × 4\n   Continent Country      Measurement Value\n   <chr>     <chr>        <chr>       <dbl>\n 1 Africa    Algeria      Langs          18\n 2 Africa    Angola       Langs          42\n 3 Oceania   Australia    Langs         234\n 4 Asia      Bangladesh   Langs          37\n 5 Africa    Benin        Langs          52\n 6 Americas  Bolivia      Langs          38\n 7 Africa    Botswana     Langs          27\n 8 Americas  Brazil       Langs         209\n 9 Africa    Burkina Faso Langs          75\n10 Africa    CAR          Langs          94\n# … with 434 more rows\n```\n:::\n:::\n\n\n::: callout-note\n## Work it out\n\nUsing a `pivot _*()` functions, convert the `untidydata::language_diversity` data to the format we've seen the Nettle data in.\n:::\n\n## Stretch goals\n\nHere's Spanish vowel data, also from `{untidydata}`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspanish_vowels\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 750 × 4\n   label        rep    f1    f2\n   <chr>      <int> <dbl> <dbl>\n 1 p01-male-a     1  615. 1231.\n 2 p01-male-a     2  645. 1282.\n 3 p01-male-a     3  608. 1248.\n 4 p01-male-e     1  477. 1612.\n 5 p01-male-e     2  457. 1839.\n 6 p01-male-e     3  445. 1849.\n 7 p01-male-i     1  309. 2153.\n 8 p01-male-i     2  259. 2176.\n 9 p01-male-i     3  337. 2015.\n10 p01-male-o     1  478.  865.\n# … with 740 more rows\n```\n:::\n:::\n\n\nThe `spanish_vowels$label` column has three different variable smushed together: speaker id, speaker gender, vowel class. This is good *file naming* convention. Poor *data column* convention.\n\n::: callout-note\n## Work it out\n\nLook over the docs for `tidyr::separate()` and try to get the speaker id, speaker gender, and vowel class separated out into their own columns.\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
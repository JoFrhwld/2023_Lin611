[
  {
    "objectID": "class_notes/2023-02-09_distributions/index.html",
    "href": "class_notes/2023-02-09_distributions/index.html",
    "title": "Distributions 2",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nemo &lt;- read_csv(\"https://raw.githubusercontent.com/bodowinter/applied_statistics_book_data/master/warriner_2013_emotional_valence.csv\")\n\nRows: 13915 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Word\ndbl (1): Val\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\num &lt;- read_tsv(\"https://bit.ly/3JdeSbx\")\n\nRows: 26060 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): word, next_seg, idstring\ndbl (11): start_time, end_time, vowel_start, vowel_end, nasal_start, nasal_e...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "class_notes/2023-02-09_distributions/index.html#discrete-probability-distributions",
    "href": "class_notes/2023-02-09_distributions/index.html#discrete-probability-distributions",
    "title": "Distributions 2",
    "section": "Discrete Probability distributions",
    "text": "Discrete Probability distributions\nOur dice rolling experiment is an example of a discrete probability distribution. This code chunk plots the predicted probability of observing \\(n\\) ⚀ or ⚅ on 10 rolls.\n\n## Define how many rolls\nnrolls &lt;- 10\n\n## get the probability in a dataframe\ntibble(\n  ## possible observations\n  observed = 0:nrolls,\n  ## dbinom for the probability distribution\n  prob = dbinom(\n    ## possible observations\n    x = observed,\n    ## number of rolls\n    size = nrolls,\n    ## probability of success\n    prob = 2/6\n  )\n) |&gt; \n  ## the plotting code\n  ggplot(aes(x = observed, y = prob))+\n    geom_col()\n\n\n\n\nIf we have more rolls, the range of probability density, relative to the whole range of possible observations, decreases.\n\n## Define how many rolls\nnrolls &lt;- 1000\n\n## get the probability in a dataframe\ntibble(\n  ## possible observations\n  observed = 0:nrolls,\n  ## dbinom for the probability distribution\n  prob = dbinom(\n    ## possible observations\n    x = observed,\n    ## number of rolls\n    size = nrolls,\n    ## probability of success\n    prob = 2/6\n  )\n) |&gt; \n  ## the plotting code\n  ggplot(aes(x = observed, y = prob))+\n    geom_col()\n\n\n\n\nWe can simulate the experiment, varying the number of groups who rolled different number of n times. Increasing the number of groups rolling a die fills in the density distribution more, but only increasing the number of rolls per group narrows the density range.\n\ncomplicated code just to make the plotset.seed(611)\nexpand_grid(\n  nrolls = c(10, 100, 1000),\n  groups = c(5, 50, 500),\n  min = 0\n) |&gt; \n  mutate(\n    expected = nrolls * (2/6)\n  )-&gt; blank\n\n## Create all combinations of rolls\n## and number of groups\nexpand_grid(\n  nrolls = c(10, 100, 1000),\n  groups = c(5, 50, 500)\n) |&gt; \n  ## We didn't learn about code like this,\n  ## using it just to make the graph\n  mutate(\n    roll_df = map2(\n      nrolls, \n      groups, \n      \\(roll, group) tibble(obs = rbinom(n = group, size = roll, prob = 2/6)))\n  ) |&gt; \n  unnest(roll_df) |&gt; \n  ggplot(aes(obs))+\n    stat_bin(binwidth = 1,\n             aes(y = after_stat(ndensity))) +\n    geom_blank(data = blank, aes(x = nrolls, y = 0))+\n    geom_blank(data = blank, aes(x = min, y = 0))+\n    geom_vline(data = blank, aes(xintercept = expected), color = \"steelblue\")+\n    facet_grid(groups~nrolls, \n               label = label_both,\n               scales = \"free\")\n\n\n\ncomplicated code just to make the plot# tibble(\n#   rand = rbinom(n = 500, size = nrolls, prob = 2/6)\n# ) |&gt; \n#   ggplot(aes(x = rand)) +\n#     stat_bin(binwidth = 1)+\n#     geom_vline(xintercept = (2/6) * nrolls, color = \"red\")+\n#     expand_limits(x = c(0, nrolls))"
  },
  {
    "objectID": "class_notes/2023-02-09_distributions/index.html#normal-distribution",
    "href": "class_notes/2023-02-09_distributions/index.html#normal-distribution",
    "title": "Distributions 2",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThe “Normal Distribution” or “Gaussian Distribution” is a continuous probability density function. Rather than the height of the curve representing the “point probability” of a value, it represents an area that sums to 1. To get the probability of a value falling within a range of the normal distribution, you sum the area under the curve.\n\ntibble(\n  x = seq(-4, 4, length = 100),\n  dens = dnorm(x, mean = 0, sd = 1)\n) |&gt;\n  ggplot(aes(x = x, y = dens)) +\n    geom_area()\n\n\n\n\n\ncomplex code just to make the plotone_sd_prob = round(pnorm(1) - pnorm(-1), digits = 2)\ntwo_four_prob = round(pnorm(4) - pnorm(2), digits = 2)\n\ntibble(\n  x = seq(-4, 4, length = 100),\n  dens = dnorm(x, mean = 0, sd = 1)\n) |&gt;\n  ggplot(aes(x = x, y = dens)) +\n    geom_area() +\n    annotate(\n      ymin = -Inf,\n      ymax = Inf,\n      xmin = -1,\n      xmax = 1,\n      geom = \"rect\",\n      alpha = 0.3,\n      fill = \"steelblue\"\n    )+\n    annotate(\n      x = 0,\n      y = 0.2,\n      label = \n        str_wrap(\n          glue::glue(\"Area under the curve between -1 and 1 = {one_sd_prob}\"),\n          15\n        ),\n      geom = \"label\",\n      color = \"steelblue\"\n    )+\n    annotate(\n      ymin = -Inf,\n      ymax = Inf,\n      xmin = 2,\n      xmax = 4,\n      geom = \"rect\",\n      alpha = 0.3,\n      fill = \"#BE3455\"\n    )+\n    annotate(\n      x = 3,\n      y = 0.2,\n      label = \n        str_wrap(\n          glue::glue(\"Area under the curve between 2 and 4 = {two_four_prob}\"),\n          15\n        ),\n      geom = \"label\",\n      color = \"#BE3455\"\n    )\n\n\n\n\nOne way to view how much area is under the curve, as you move from left to right, is to look at the “cumulative probability function”\n\ntibble(\n  prob = pnorm(\n    q = seq(-4, 4, length = 100),\n    mean = 0,\n    sd = 1\n  ),\n  x = seq(-4, 4, length = 100)\n) |&gt; \n  ggplot(aes(x = x, y = prob))+\n    geom_area()"
  },
  {
    "objectID": "class_notes/2023-02-09_distributions/index.html#real-data",
    "href": "class_notes/2023-02-09_distributions/index.html#real-data",
    "title": "Distributions 2",
    "section": "Real Data",
    "text": "Real Data\nReal data is often a bit off from being normal, but might be normal-ish\n\nemo_xbar &lt;- mean(emo$Val)\nemo_sd &lt;- sd(emo$Val)\n\nemo |&gt; \n  ggplot(aes(x = Val))+\n    stat_density()+\n    stat_function(\n      fun = dnorm,\n      args = list(mean = emo_xbar, sd = emo_sd),\n      color = \"steelblue\",\n      linewidth = 1) +\n    annotate(x = 7, y = 0.3,\n             label = \"empirical density\",\n             geom = \"label\") +\n    annotate(x = 2.5, y = 0.2,\n             label = \"normal density\",\n             geom = \"label\",\n             color = \"steelblue\")\n\n\n\n\nSome definitions\n\nmean\n\nThe sum of all values, divided by the number of observations\n\nmedian\n\nThe value at which 50% of the data is less than it, and 50% is more.\n\n\nThe mean and median are the same for symmetrical distributions, like the normal distribution.\n\nmean(emo$Val)\n\n[1] 5.063847\n\nmedian(emo$Val)\n\n[1] 5.2\n\n\n\nmodel &lt;- lm(Val ~ 1, data = emo)\nsummary(model)\n\n\nCall:\nlm(formula = Val ~ 1, data = emo)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8038 -0.8138  0.1362  0.8862  3.4662 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.06385    0.01081   468.5   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.275 on 13914 degrees of freedom"
  },
  {
    "objectID": "class_notes/2023-02-16_linear-r/index.html",
    "href": "class_notes/2023-02-16_linear-r/index.html",
    "title": "Linear Models in R",
    "section": "",
    "text": "Grab the data\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nfreq &lt;- read_csv(\"https://raw.githubusercontent.com/bodowinter/applied_statistics_book_data/master/ELP_frequency.csv\")\n\nRows: 12 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Word\ndbl (3): Freq, Log10Freq, RT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nmod &lt;- lm(RT ~ Log10Freq, data = freq)\n\n\n\n\nReuseCC-BY-SA 4.0"
  },
  {
    "objectID": "class_notes/2023-03-09_categorical-predictors/index.html",
    "href": "class_notes/2023-03-09_categorical-predictors/index.html",
    "title": "Categorical Predictors",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(forcats)\nlibrary(marginaleffects)\n\n\num_uh &lt;- read_tsv(\"https://bit.ly/3JdeSbx\") \n\nRows: 26060 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): word, next_seg, idstring\ndbl (11): start_time, end_time, vowel_start, vowel_end, nasal_start, nasal_e...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\num_uh\n\n# A tibble: 26,060 × 14\n   word  start…¹ end_t…² vowel…³ vowel…⁴ nasal…⁵ nasal…⁶ next_…⁷ next_…⁸ next_…⁹\n   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 UH       24.4    24.7    24.4    24.7    NA      NA   S          24.7    24.9\n 2 UH       35.0    35.2    35.0    35.2    NA      NA   F          35.2    35.4\n 3 UM       37.9    38.3    37.9    38.1    38.1    38.3 sp         38.3    38.4\n 4 UH       44.5    44.7    44.5    44.7    NA      NA   DH         44.7    44.7\n 5 UH       57.6    57.8    57.6    57.8    NA      NA   AY1        57.8    57.9\n 6 UH       62.3    62.5    62.3    62.5    NA      NA   sp         62.5    63.0\n 7 UH       73.9    74.2    73.9    74.2    NA      NA   sp         74.2    75.0\n 8 UH       75.1    75.4    75.1    75.4    NA      NA   sp         75.4    75.7\n 9 UM       81.6    82.0    81.6    81.8    81.8    82.0 sp         82.0    84.0\n10 UH       92.6    92.9    92.6    92.9    NA      NA   sp         92.9    93.4\n# … with 26,050 more rows, 4 more variables: chunk_start &lt;dbl&gt;,\n#   chunk_end &lt;dbl&gt;, nwords &lt;dbl&gt;, idstring &lt;chr&gt;, and abbreviated variable\n#   names ¹​start_time, ²​end_time, ³​vowel_start, ⁴​vowel_end, ⁵​nasal_start,\n#   ⁶​nasal_end, ⁷​next_seg, ⁸​next_seg_start, ⁹​next_seg_end\n\n\n\num_uh |&gt; \n  mutate(\n    vowel_dur = vowel_end - vowel_start,\n        fol_pause = case_when(\n          next_seg == \"sp\" ~ \"pause\", \n          .default = \"no pause\"\n        )\n  ) |&gt; \n  select(idstring, word, fol_pause, vowel_dur) -&gt; \n  pause_data\n\n\n\n\nReuseCC-BY-SA 4.0"
  },
  {
    "objectID": "class_notes/2023-03-23_interactions/index.html",
    "href": "class_notes/2023-03-23_interactions/index.html",
    "title": "Interactions",
    "section": "",
    "text": "Important\n\n\n\nThe axis labels will only show in light mode!\n\n\n\n\n\n\nviewof slope_a = Inputs.range([-5, 5], {step: 0.01, label: \"a slope \"});\n\n\n\n\n\n\n\nviewof intercept_a = Inputs.range([-20, 20], {step: 1, label: \"a intercept\"});\n\n\n\n\n\n\n\n\n\nviewof slope_b = Inputs.range([-5, 5], {step: 0.01, label: \"b slope \"});\nviewof intercept_b = Inputs.range([-20, 20], {step: 1, label: \"b intercept\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof x_center = Inputs.range([-5, 5], {step: 0.01, label: \"x centering\"});\n\n\n\n\n\n\n\n\n\n\nmaximal = (5 * 5) + 20;\n\n\na_at_x = (x_center * slope_a) +intercept_a;\nb_at_x = (x_center * slope_b) +intercept_b;\na_at_x1 = ((x_center+1) * slope_a) +intercept_a;\nb_at_x1 = ((x_center+1) * slope_b) +intercept_b;\n\n\n\nb_eff = b_at_x - a_at_x;\nb_counter = b_at_x + slope_a;\ninteraction =  b_at_x1 - b_counter;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmyline = [\n  {x: -5, y: (-5 * slope_a)+intercept_a, group: \"a\"},\n  {x: 5, y: (5 * slope_a)+intercept_a, group: \"a\"},\n  {x: -5, y: (-5 * slope_b)+intercept_b, group: \"b\"},\n  {x: 5, y: (5 * slope_b)+intercept_b, group: \"b\"}\n]\n\n\n\n\n\n\n\ninterceptPoint = [\n  {x: x_center, y: (x_center * slope_a) +intercept_a}\n];\n\narrowLine = [\n  {\n    from_x: x_center,\n    to_x: x_center,\n    from_y: (x_center * slope_a) +intercept_a,\n    to_y: (x_center * slope_b) +intercept_b\n  }\n]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na_step = [\n  {x: x_center, y: (x_center * slope_a) +intercept_a, group: \"a\"},\n  //{x: x_center, y: (x_center+1 * slope_a) +intercept_a},\n  {x: x_center+1, y: ((x_center+1) * slope_a) + intercept_a, group: \"a\"}\n];\n\nb_step = [\n  {x: x_center, y: b_at_x, group: \"a\"},\n  //{x: x_center, y: (x_center+1 * slope_a) +intercept_a},\n  {x: x_center+1, y: b_at_x + slope_a, group: \"a\"}\n];\n\nb_interaction = [\n  {from_x: x_center+1,\n   to_x: x_center+1,\n   from_y: b_counter,\n   to_y: b_at_x1,\n   group: \"b\"\n  }\n];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  y: {domain: [-maximal, maximal]},\n  x: {domain: [-5, 6]},\n  marks: [\n    Plot.lineY(myline, {x: \"x\", y: \"y\", stroke: \"group\", strokeWidth: 4}),\n    Plot.arrow(arrowLine,\n      {\n        x1: \"from_x\",\n        x2: \"to_x\",\n        y1: \"from_y\",\n        y2: \"to_y\",\n        stroke: \"red\",\n        strokeWidth: 3\n      }\n    ),\n    Plot.arrow(b_interaction,\n      {\n        x1: \"from_x\",\n        x2: \"to_x\",\n        y1: \"from_y\",\n        y2: \"to_y\",\n        stroke: \"group\",\n        strokeWidth: 3\n      }\n    ),\n    Plot.line(a_step,\n      {\n        x: \"x\",\n        y: \"y\",\n        curve: \"step-after\",\n        stroke: \"group\",\n        strokeDasharray: [2],\n        strokeWidth: 2\n      }\n    ),\n    Plot.line(b_step,\n      {\n        x: \"x\",\n        y: \"y\",\n        curve: \"step-after\",\n        stroke: \"group\",\n        strokeDasharray: [5],\n        strokeWidth: 2\n      }\n    ),\n    Plot.dot(interceptPoint, {x: \"x\", y: \"y\", fill: \"black\", r: 5}),\n    Plot.text(myline, {\n      filter: d =&gt; d.x === 5,\n      x: 4.8,\n      y: \"y\",\n      text: \"group\",\n      fill: \"group\",\n      fontSize: 20\n    })\n  ]\n})\n\n\n\n\n\n\n\nWhat you’d see in the regression results for lm(y~x*group)\n\nregressionTable = [\n  {\n    term: \"Intercept\",\n    estimate: a_at_x,\n    symbol: \"black dot\"\n  },\n  {\n    term: \"x\",\n    estimate: slope_a,\n    symbol: \"dashed blue step\"\n  },\n  {\n    term: \"groupb\",\n    estimate: (b_at_x - a_at_x),\n    symbol: \"red arrow\"\n  },\n  {\n    term: \"x:groupb\",\n    estimate: (slope_b - slope_a),\n    symbol: \"orange arrow\"\n  }\n];\nInputs.table(regressionTable, {sort: false})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC-BY-SA 4.0"
  },
  {
    "objectID": "class_notes/2023-01-26_tidyverse2/index.html",
    "href": "class_notes/2023-01-26_tidyverse2/index.html",
    "title": "Continuing with the tidyverse",
    "section": "",
    "text": "library(tidyverse)\n\nA useful diagram for understanding joins https://r4ds.hadley.nz/joins.html#fig-join-left\n\n\n\n\n\n\nWork it out\n\n\n\n\num &lt;- read_tsv(\"https://bit.ly/3JdeSbx\")\ndemo &lt;- read_csv(\"https://bit.ly/3wOfcGx\")\n\nJoin these two datasets together (dplyr::left_join()) and find out\n\nWhich filled pause did people born before 1930 use the most?\nWhich filled pause did people born after 1980 use the most?\n\nThink of using functions like\n\ndplyr::filter()\ndplyr::count()\ndplyr::arrange()"
  },
  {
    "objectID": "class_notes/2023-01-26_tidyverse2/index.html#joins",
    "href": "class_notes/2023-01-26_tidyverse2/index.html#joins",
    "title": "Continuing with the tidyverse",
    "section": "",
    "text": "library(tidyverse)\n\nA useful diagram for understanding joins https://r4ds.hadley.nz/joins.html#fig-join-left\n\n\n\n\n\n\nWork it out\n\n\n\n\num &lt;- read_tsv(\"https://bit.ly/3JdeSbx\")\ndemo &lt;- read_csv(\"https://bit.ly/3wOfcGx\")\n\nJoin these two datasets together (dplyr::left_join()) and find out\n\nWhich filled pause did people born before 1930 use the most?\nWhich filled pause did people born after 1980 use the most?\n\nThink of using functions like\n\ndplyr::filter()\ndplyr::count()\ndplyr::arrange()"
  },
  {
    "objectID": "class_notes/2023-01-26_tidyverse2/index.html#pivots",
    "href": "class_notes/2023-01-26_tidyverse2/index.html#pivots",
    "title": "Continuing with the tidyverse",
    "section": "Pivots",
    "text": "Pivots\n\ninstall.packages(\"nasapower\")\n\n\nlibrary(nasapower)\n\nGetting monthly temperature data\n\nlex_temp &lt;- \n  get_power(\n    community = \"ag\",\n    temporal_api = \"monthly\",\n    pars = \"T2M\",\n    dates = c(\"1985-01-01\", \"2021-12-31\"),\n    lonlat = c(-84.501640,  38.047989)\n  )\n\nHere is monthly temperature data for Lexington according to NASA\n\nlex_temp\n\nNASA/POWER CERES/MERRA2 Native Resolution Monthly and Annual  \n Dates (month/day/year): 01/01/1985 through 12/31/2021  \n Location: Latitude  38.048   Longitude -84.5016  \n Elevation from MERRA-2: Average for 0.5 x 0.625 degree lat/lon region = 280.65 meters \n The value for missing source data that cannot be computed or is outside of the sources availability range: NA  \n Parameter(s):  \n \n Parameters: \n T2M     MERRA-2 Temperature at 2 Meters (C)  \n \n# A tibble: 37 × 17\n     LON   LAT PARAMETER  YEAR   JAN   FEB   MAR   APR   MAY   JUN   JUL   AUG\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 -84.5  38.0 T2M        1985 -4.87 -3.06  7.64  13.5  17.4  21.0  23.8  23.1\n 2 -84.5  38.0 T2M        1986 -1.52  1.77  6.53  12.8  18.1  23.4  26.5  23.4\n 3 -84.5  38.0 T2M        1987 -0.99  1.08  6.54  10.8  19.7  22.8  25.4  25.7\n 4 -84.5  38.0 T2M        1988 -2.38 -0.56  5.58  11.0  17.2  23.6  26.4  26.2\n 5 -84.5  38.0 T2M        1989  2.42 -0.93  6.44  10.7  14.7  21.1  23.5  23.1\n 6 -84.5  38.0 T2M        1990  2.61  4.76  7.74  10.8  15.8  21.3  23.7  23.8\n 7 -84.5  38.0 T2M        1991 -0.54  2.13  7.15  13.5  20.4  22.3  24.3  24.3\n 8 -84.5  38.0 T2M        1992  0.09  3.51  6.23  11.7  15.9  19.8  23.6  20.9\n 9 -84.5  38.0 T2M        1993  1.78 -0.27  4.3   10.6  17.4  21.8  26.3  24.8\n10 -84.5  38.0 T2M        1994 -3.89  0.8   4.54  13.2  15.1  22.7  24.3  23.2\n# … with 27 more rows, and 5 more variables: SEP &lt;dbl&gt;, OCT &lt;dbl&gt;, NOV &lt;dbl&gt;,\n#   DEC &lt;dbl&gt;, ANN &lt;dbl&gt;\n\n\nPivoting long\n\nlex_long &lt;- \n  lex_temp |&gt; \n  pivot_longer(\n    cols = JAN:DEC,\n    names_to = \"month\",\n    values_to = \"temp\"\n  )\n\nlex_long\n\n# A tibble: 444 × 7\n     LON   LAT PARAMETER  YEAR   ANN month  temp\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 -84.5  38.0 T2M        1985  12.0 JAN   -4.87\n 2 -84.5  38.0 T2M        1985  12.0 FEB   -3.06\n 3 -84.5  38.0 T2M        1985  12.0 MAR    7.64\n 4 -84.5  38.0 T2M        1985  12.0 APR   13.5 \n 5 -84.5  38.0 T2M        1985  12.0 MAY   17.4 \n 6 -84.5  38.0 T2M        1985  12.0 JUN   21.0 \n 7 -84.5  38.0 T2M        1985  12.0 JUL   23.8 \n 8 -84.5  38.0 T2M        1985  12.0 AUG   23.1 \n 9 -84.5  38.0 T2M        1985  12.0 SEP   19.8 \n10 -84.5  38.0 T2M        1985  12.0 OCT   15.5 \n# … with 434 more rows\n\n\nPivoting wide\nYou can do data operations, and then pivot the data back to wide.\n\nlex_long |&gt; \n  # converting year to a decade value\n  mutate(decade = floor(YEAR / 10)) |&gt; \n  # grouping by decade and month\n  group_by(decade, month) |&gt; \n  # getting average temperature within groups\n  summarise(mean_temp = mean(temp)) |&gt; \n  # pivoting wide\n  pivot_wider(\n    names_from = month,\n    values_from = mean_temp\n  )\n\n`summarise()` has grouped output by 'decade'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 5 × 13\n# Groups:   decade [5]\n  decade   APR   AUG    DEC   FEB    JAN   JUL   JUN   MAR   MAY   NOV   OCT\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    198  11.8  24.3 -0.564 -0.34 -1.47   25.1  22.4  6.55  17.4  7.50  12.4\n2    199  11.6  23.7  1.98   2.14  0.077  24.4  21.8  5.72  17.2  6.16  13.1\n3    200  12.7  24.5  1.08   1.23  0.158  24.1  22.3  6.86  17.7  7.34  13.6\n4    201  12.9  24.1  2.46   1.47 -0.838  24.7  22.7  6.26  18.6  6.08  14.0\n5    202  11.2  24.2  4.20   0.68  1.48   24.8  22.2  9.12  16.2  6.88  15.0\n# … with 1 more variable: SEP &lt;dbl&gt;\n\n\n\n\n\n\n\n\nWork it out\n\n\n\nWith the “UM” data\n\nCalculate the average duration of the vowel for each person for each word. Think of using functions like\n\ndplyr::mutate()\ndplyr::group_by()\ndplyr::summarise()\n\n\nFigure out how many times longer the vowel is in “UM” than for “UH” for each person. Think of using functions like\n\ntidyr::pivot_wider()\ndplyr::mutate()"
  },
  {
    "objectID": "class_notes/2023-01-26_tidyverse2/index.html#untidy-data-examples",
    "href": "class_notes/2023-01-26_tidyverse2/index.html#untidy-data-examples",
    "title": "Continuing with the tidyverse",
    "section": "Untidy data examples",
    "text": "Untidy data examples\nSource: untidydata by Joseph Casillas\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"jvcasillas/untidydata\")\n\n\nlibrary(untidydata)\n\nThis is just the nettle dataset all over again, but starts out “long”\n\nlanguage_diversity\n\n# A tibble: 444 × 4\n   Continent Country      Measurement Value\n   &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;\n 1 Africa    Algeria      Langs          18\n 2 Africa    Angola       Langs          42\n 3 Oceania   Australia    Langs         234\n 4 Asia      Bangladesh   Langs          37\n 5 Africa    Benin        Langs          52\n 6 Americas  Bolivia      Langs          38\n 7 Africa    Botswana     Langs          27\n 8 Americas  Brazil       Langs         209\n 9 Africa    Burkina Faso Langs          75\n10 Africa    CAR          Langs          94\n# … with 434 more rows\n\n\n\n\n\n\n\n\nWork it out\n\n\n\nUsing a pivot _*() functions, convert the untidydata::language_diversity data to the format we’ve seen the Nettle data in."
  },
  {
    "objectID": "class_notes/2023-01-26_tidyverse2/index.html#stretch-goals",
    "href": "class_notes/2023-01-26_tidyverse2/index.html#stretch-goals",
    "title": "Continuing with the tidyverse",
    "section": "Stretch goals",
    "text": "Stretch goals\nHere’s Spanish vowel data, also from untidydata.\n\nspanish_vowels\n\n# A tibble: 750 × 4\n   label        rep    f1    f2\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 p01-male-a     1  615. 1231.\n 2 p01-male-a     2  645. 1282.\n 3 p01-male-a     3  608. 1248.\n 4 p01-male-e     1  477. 1612.\n 5 p01-male-e     2  457. 1839.\n 6 p01-male-e     3  445. 1849.\n 7 p01-male-i     1  309. 2153.\n 8 p01-male-i     2  259. 2176.\n 9 p01-male-i     3  337. 2015.\n10 p01-male-o     1  478.  865.\n# … with 740 more rows\n\n\nThe spanish_vowels$label column has three different variable smushed together: speaker id, speaker gender, vowel class. This is good file naming convention. Poor data column convention.\n\n\n\n\n\n\nWork it out\n\n\n\nLook over the docs for tidyr::separate() and try to get the speaker id, speaker gender, and vowel class separated out into their own columns."
  },
  {
    "objectID": "class_notes/2023-02-02_tidynorm/index.html",
    "href": "class_notes/2023-02-02_tidynorm/index.html",
    "title": "Tidy Vowel Normalization",
    "section": "",
    "text": "library(tidyverse)\nlibrary(khroma)\ntheme_set(theme_minimal())\n\n\nlibrary(untidydata)\n\n\ntidy_vowels &lt;-\n  spanish_vowels |&gt;\n  separate(\n    col = label,\n    sep = \"-\",\n    into = c(\"id\", \"gender\", \"vowel\")\n  ) |&gt; \n  # Recoding gender labels\n  mutate(\n    gender = case_when(\n      gender == \"female\" ~ \"women\",\n      gender == \"male\" ~ \"men\"\n    )\n  )\n\n\ntidy_vowels |&gt; \n  ggplot(aes(f2, f1, color = gender)) +\n    stat_density2d()+\n    scale_x_continuous(trans = \"reverse\")+\n    scale_y_continuous(trans = \"reverse\")+\n    scale_color_brewer(palette = \"Dark2\")"
  },
  {
    "objectID": "class_notes/2023-02-02_tidynorm/index.html#setup-and-plot-original-data",
    "href": "class_notes/2023-02-02_tidynorm/index.html#setup-and-plot-original-data",
    "title": "Tidy Vowel Normalization",
    "section": "",
    "text": "library(tidyverse)\nlibrary(khroma)\ntheme_set(theme_minimal())\n\n\nlibrary(untidydata)\n\n\ntidy_vowels &lt;-\n  spanish_vowels |&gt;\n  separate(\n    col = label,\n    sep = \"-\",\n    into = c(\"id\", \"gender\", \"vowel\")\n  ) |&gt; \n  # Recoding gender labels\n  mutate(\n    gender = case_when(\n      gender == \"female\" ~ \"women\",\n      gender == \"male\" ~ \"men\"\n    )\n  )\n\n\ntidy_vowels |&gt; \n  ggplot(aes(f2, f1, color = gender)) +\n    stat_density2d()+\n    scale_x_continuous(trans = \"reverse\")+\n    scale_y_continuous(trans = \"reverse\")+\n    scale_color_brewer(palette = \"Dark2\")"
  },
  {
    "objectID": "class_notes/2023-02-02_tidynorm/index.html#unnormalized-vowels",
    "href": "class_notes/2023-02-02_tidynorm/index.html#unnormalized-vowels",
    "title": "Tidy Vowel Normalization",
    "section": "Unnormalized vowels",
    "text": "Unnormalized vowels\n\ntidy_vowels |&gt; \n  ggplot(aes(x = gender))+\n    geom_violin(aes(y = f1, fill = \"f1\"))+\n    geom_violin(aes(y = f2, fill = \"f2\")) +\n    scale_y_continuous(trans = \"log10\")+\n    scale_fill_bright()+\n    labs(y = \"frequency (hz)\")"
  },
  {
    "objectID": "class_notes/2023-02-02_tidynorm/index.html#normalizing",
    "href": "class_notes/2023-02-02_tidynorm/index.html#normalizing",
    "title": "Tidy Vowel Normalization",
    "section": "Normalizing",
    "text": "Normalizing\nz-scoring a.k.a. “Lobanov”\nLog-mean\nLog-mean normalization, following Barreda (2021)."
  },
  {
    "objectID": "class_notes/2023-01-10/index.html",
    "href": "class_notes/2023-01-10/index.html",
    "title": "Welcome Day",
    "section": "",
    "text": "An illustration of R vs RStudio vs Quarto\n\n\nR (on the left) is a programming language, which looks at (👀) R scipt files (the box with .r) and interprets them. RStudio is an IDE (Integrated Development Environment) that provides a more pleasant interface to working with R.\nQuarto (not pictured) is a program that interprets Quarto documents (the box with .qmd) and renders them as html files. We'll be doing this all inside of RStudio."
  },
  {
    "objectID": "class_notes/2023-01-10/index.html#r-vs-rstudio-vs-quarto",
    "href": "class_notes/2023-01-10/index.html#r-vs-rstudio-vs-quarto",
    "title": "Welcome Day",
    "section": "",
    "text": "An illustration of R vs RStudio vs Quarto\n\n\nR (on the left) is a programming language, which looks at (👀) R scipt files (the box with .r) and interprets them. RStudio is an IDE (Integrated Development Environment) that provides a more pleasant interface to working with R.\nQuarto (not pictured) is a program that interprets Quarto documents (the box with .qmd) and renders them as html files. We'll be doing this all inside of RStudio."
  },
  {
    "objectID": "class_notes/2023-01-10/index.html#examples-from-inside-class",
    "href": "class_notes/2023-01-10/index.html#examples-from-inside-class",
    "title": "Welcome Day",
    "section": "Examples from inside class",
    "text": "Examples from inside class\nThis was an example of inserting an R code chunk into a quarto notebook\n\n```{r}\n1+1\n```\n\n[1] 2\n\n\nThis was an example of making a plot in quarto\n\n```{r}\nplot(cars)\n```\n\n\n\n\nThis was an example of running R code from Winters (2019). This particular code prints out all of the named colors in R.\n\n```{r}\ncolors()\n```\n\n  [1] \"white\"                \"aliceblue\"            \"antiquewhite\"        \n  [4] \"antiquewhite1\"        \"antiquewhite2\"        \"antiquewhite3\"       \n  [7] \"antiquewhite4\"        \"aquamarine\"           \"aquamarine1\"         \n [10] \"aquamarine2\"          \"aquamarine3\"          \"aquamarine4\"         \n [13] \"azure\"                \"azure1\"               \"azure2\"              \n [16] \"azure3\"               \"azure4\"               \"beige\"               \n [19] \"bisque\"               \"bisque1\"              \"bisque2\"             \n [22] \"bisque3\"              \"bisque4\"              \"black\"               \n [25] \"blanchedalmond\"       \"blue\"                 \"blue1\"               \n [28] \"blue2\"                \"blue3\"                \"blue4\"               \n [31] \"blueviolet\"           \"brown\"                \"brown1\"              \n [34] \"brown2\"               \"brown3\"               \"brown4\"              \n [37] \"burlywood\"            \"burlywood1\"           \"burlywood2\"          \n [40] \"burlywood3\"           \"burlywood4\"           \"cadetblue\"           \n [43] \"cadetblue1\"           \"cadetblue2\"           \"cadetblue3\"          \n [46] \"cadetblue4\"           \"chartreuse\"           \"chartreuse1\"         \n [49] \"chartreuse2\"          \"chartreuse3\"          \"chartreuse4\"         \n [52] \"chocolate\"            \"chocolate1\"           \"chocolate2\"          \n [55] \"chocolate3\"           \"chocolate4\"           \"coral\"               \n [58] \"coral1\"               \"coral2\"               \"coral3\"              \n [61] \"coral4\"               \"cornflowerblue\"       \"cornsilk\"            \n [64] \"cornsilk1\"            \"cornsilk2\"            \"cornsilk3\"           \n [67] \"cornsilk4\"            \"cyan\"                 \"cyan1\"               \n [70] \"cyan2\"                \"cyan3\"                \"cyan4\"               \n [73] \"darkblue\"             \"darkcyan\"             \"darkgoldenrod\"       \n [76] \"darkgoldenrod1\"       \"darkgoldenrod2\"       \"darkgoldenrod3\"      \n [79] \"darkgoldenrod4\"       \"darkgray\"             \"darkgreen\"           \n [82] \"darkgrey\"             \"darkkhaki\"            \"darkmagenta\"         \n [85] \"darkolivegreen\"       \"darkolivegreen1\"      \"darkolivegreen2\"     \n [88] \"darkolivegreen3\"      \"darkolivegreen4\"      \"darkorange\"          \n [91] \"darkorange1\"          \"darkorange2\"          \"darkorange3\"         \n [94] \"darkorange4\"          \"darkorchid\"           \"darkorchid1\"         \n [97] \"darkorchid2\"          \"darkorchid3\"          \"darkorchid4\"         \n[100] \"darkred\"              \"darksalmon\"           \"darkseagreen\"        \n[103] \"darkseagreen1\"        \"darkseagreen2\"        \"darkseagreen3\"       \n[106] \"darkseagreen4\"        \"darkslateblue\"        \"darkslategray\"       \n[109] \"darkslategray1\"       \"darkslategray2\"       \"darkslategray3\"      \n[112] \"darkslategray4\"       \"darkslategrey\"        \"darkturquoise\"       \n[115] \"darkviolet\"           \"deeppink\"             \"deeppink1\"           \n[118] \"deeppink2\"            \"deeppink3\"            \"deeppink4\"           \n[121] \"deepskyblue\"          \"deepskyblue1\"         \"deepskyblue2\"        \n[124] \"deepskyblue3\"         \"deepskyblue4\"         \"dimgray\"             \n[127] \"dimgrey\"              \"dodgerblue\"           \"dodgerblue1\"         \n[130] \"dodgerblue2\"          \"dodgerblue3\"          \"dodgerblue4\"         \n[133] \"firebrick\"            \"firebrick1\"           \"firebrick2\"          \n[136] \"firebrick3\"           \"firebrick4\"           \"floralwhite\"         \n[139] \"forestgreen\"          \"gainsboro\"            \"ghostwhite\"          \n[142] \"gold\"                 \"gold1\"                \"gold2\"               \n[145] \"gold3\"                \"gold4\"                \"goldenrod\"           \n[148] \"goldenrod1\"           \"goldenrod2\"           \"goldenrod3\"          \n[151] \"goldenrod4\"           \"gray\"                 \"gray0\"               \n[154] \"gray1\"                \"gray2\"                \"gray3\"               \n[157] \"gray4\"                \"gray5\"                \"gray6\"               \n[160] \"gray7\"                \"gray8\"                \"gray9\"               \n[163] \"gray10\"               \"gray11\"               \"gray12\"              \n[166] \"gray13\"               \"gray14\"               \"gray15\"              \n[169] \"gray16\"               \"gray17\"               \"gray18\"              \n[172] \"gray19\"               \"gray20\"               \"gray21\"              \n[175] \"gray22\"               \"gray23\"               \"gray24\"              \n[178] \"gray25\"               \"gray26\"               \"gray27\"              \n[181] \"gray28\"               \"gray29\"               \"gray30\"              \n[184] \"gray31\"               \"gray32\"               \"gray33\"              \n[187] \"gray34\"               \"gray35\"               \"gray36\"              \n[190] \"gray37\"               \"gray38\"               \"gray39\"              \n[193] \"gray40\"               \"gray41\"               \"gray42\"              \n[196] \"gray43\"               \"gray44\"               \"gray45\"              \n[199] \"gray46\"               \"gray47\"               \"gray48\"              \n[202] \"gray49\"               \"gray50\"               \"gray51\"              \n[205] \"gray52\"               \"gray53\"               \"gray54\"              \n[208] \"gray55\"               \"gray56\"               \"gray57\"              \n[211] \"gray58\"               \"gray59\"               \"gray60\"              \n[214] \"gray61\"               \"gray62\"               \"gray63\"              \n[217] \"gray64\"               \"gray65\"               \"gray66\"              \n[220] \"gray67\"               \"gray68\"               \"gray69\"              \n[223] \"gray70\"               \"gray71\"               \"gray72\"              \n[226] \"gray73\"               \"gray74\"               \"gray75\"              \n[229] \"gray76\"               \"gray77\"               \"gray78\"              \n[232] \"gray79\"               \"gray80\"               \"gray81\"              \n[235] \"gray82\"               \"gray83\"               \"gray84\"              \n[238] \"gray85\"               \"gray86\"               \"gray87\"              \n[241] \"gray88\"               \"gray89\"               \"gray90\"              \n[244] \"gray91\"               \"gray92\"               \"gray93\"              \n[247] \"gray94\"               \"gray95\"               \"gray96\"              \n[250] \"gray97\"               \"gray98\"               \"gray99\"              \n[253] \"gray100\"              \"green\"                \"green1\"              \n[256] \"green2\"               \"green3\"               \"green4\"              \n[259] \"greenyellow\"          \"grey\"                 \"grey0\"               \n[262] \"grey1\"                \"grey2\"                \"grey3\"               \n[265] \"grey4\"                \"grey5\"                \"grey6\"               \n[268] \"grey7\"                \"grey8\"                \"grey9\"               \n[271] \"grey10\"               \"grey11\"               \"grey12\"              \n[274] \"grey13\"               \"grey14\"               \"grey15\"              \n[277] \"grey16\"               \"grey17\"               \"grey18\"              \n[280] \"grey19\"               \"grey20\"               \"grey21\"              \n[283] \"grey22\"               \"grey23\"               \"grey24\"              \n[286] \"grey25\"               \"grey26\"               \"grey27\"              \n[289] \"grey28\"               \"grey29\"               \"grey30\"              \n[292] \"grey31\"               \"grey32\"               \"grey33\"              \n[295] \"grey34\"               \"grey35\"               \"grey36\"              \n[298] \"grey37\"               \"grey38\"               \"grey39\"              \n[301] \"grey40\"               \"grey41\"               \"grey42\"              \n[304] \"grey43\"               \"grey44\"               \"grey45\"              \n[307] \"grey46\"               \"grey47\"               \"grey48\"              \n[310] \"grey49\"               \"grey50\"               \"grey51\"              \n[313] \"grey52\"               \"grey53\"               \"grey54\"              \n[316] \"grey55\"               \"grey56\"               \"grey57\"              \n[319] \"grey58\"               \"grey59\"               \"grey60\"              \n[322] \"grey61\"               \"grey62\"               \"grey63\"              \n[325] \"grey64\"               \"grey65\"               \"grey66\"              \n[328] \"grey67\"               \"grey68\"               \"grey69\"              \n[331] \"grey70\"               \"grey71\"               \"grey72\"              \n[334] \"grey73\"               \"grey74\"               \"grey75\"              \n[337] \"grey76\"               \"grey77\"               \"grey78\"              \n[340] \"grey79\"               \"grey80\"               \"grey81\"              \n[343] \"grey82\"               \"grey83\"               \"grey84\"              \n[346] \"grey85\"               \"grey86\"               \"grey87\"              \n[349] \"grey88\"               \"grey89\"               \"grey90\"              \n[352] \"grey91\"               \"grey92\"               \"grey93\"              \n[355] \"grey94\"               \"grey95\"               \"grey96\"              \n[358] \"grey97\"               \"grey98\"               \"grey99\"              \n[361] \"grey100\"              \"honeydew\"             \"honeydew1\"           \n[364] \"honeydew2\"            \"honeydew3\"            \"honeydew4\"           \n[367] \"hotpink\"              \"hotpink1\"             \"hotpink2\"            \n[370] \"hotpink3\"             \"hotpink4\"             \"indianred\"           \n[373] \"indianred1\"           \"indianred2\"           \"indianred3\"          \n[376] \"indianred4\"           \"ivory\"                \"ivory1\"              \n[379] \"ivory2\"               \"ivory3\"               \"ivory4\"              \n[382] \"khaki\"                \"khaki1\"               \"khaki2\"              \n[385] \"khaki3\"               \"khaki4\"               \"lavender\"            \n[388] \"lavenderblush\"        \"lavenderblush1\"       \"lavenderblush2\"      \n[391] \"lavenderblush3\"       \"lavenderblush4\"       \"lawngreen\"           \n[394] \"lemonchiffon\"         \"lemonchiffon1\"        \"lemonchiffon2\"       \n[397] \"lemonchiffon3\"        \"lemonchiffon4\"        \"lightblue\"           \n[400] \"lightblue1\"           \"lightblue2\"           \"lightblue3\"          \n[403] \"lightblue4\"           \"lightcoral\"           \"lightcyan\"           \n[406] \"lightcyan1\"           \"lightcyan2\"           \"lightcyan3\"          \n[409] \"lightcyan4\"           \"lightgoldenrod\"       \"lightgoldenrod1\"     \n[412] \"lightgoldenrod2\"      \"lightgoldenrod3\"      \"lightgoldenrod4\"     \n[415] \"lightgoldenrodyellow\" \"lightgray\"            \"lightgreen\"          \n[418] \"lightgrey\"            \"lightpink\"            \"lightpink1\"          \n[421] \"lightpink2\"           \"lightpink3\"           \"lightpink4\"          \n[424] \"lightsalmon\"          \"lightsalmon1\"         \"lightsalmon2\"        \n[427] \"lightsalmon3\"         \"lightsalmon4\"         \"lightseagreen\"       \n[430] \"lightskyblue\"         \"lightskyblue1\"        \"lightskyblue2\"       \n[433] \"lightskyblue3\"        \"lightskyblue4\"        \"lightslateblue\"      \n[436] \"lightslategray\"       \"lightslategrey\"       \"lightsteelblue\"      \n[439] \"lightsteelblue1\"      \"lightsteelblue2\"      \"lightsteelblue3\"     \n[442] \"lightsteelblue4\"      \"lightyellow\"          \"lightyellow1\"        \n[445] \"lightyellow2\"         \"lightyellow3\"         \"lightyellow4\"        \n[448] \"limegreen\"            \"linen\"                \"magenta\"             \n[451] \"magenta1\"             \"magenta2\"             \"magenta3\"            \n[454] \"magenta4\"             \"maroon\"               \"maroon1\"             \n[457] \"maroon2\"              \"maroon3\"              \"maroon4\"             \n[460] \"mediumaquamarine\"     \"mediumblue\"           \"mediumorchid\"        \n[463] \"mediumorchid1\"        \"mediumorchid2\"        \"mediumorchid3\"       \n[466] \"mediumorchid4\"        \"mediumpurple\"         \"mediumpurple1\"       \n[469] \"mediumpurple2\"        \"mediumpurple3\"        \"mediumpurple4\"       \n[472] \"mediumseagreen\"       \"mediumslateblue\"      \"mediumspringgreen\"   \n[475] \"mediumturquoise\"      \"mediumvioletred\"      \"midnightblue\"        \n[478] \"mintcream\"            \"mistyrose\"            \"mistyrose1\"          \n[481] \"mistyrose2\"           \"mistyrose3\"           \"mistyrose4\"          \n[484] \"moccasin\"             \"navajowhite\"          \"navajowhite1\"        \n[487] \"navajowhite2\"         \"navajowhite3\"         \"navajowhite4\"        \n[490] \"navy\"                 \"navyblue\"             \"oldlace\"             \n[493] \"olivedrab\"            \"olivedrab1\"           \"olivedrab2\"          \n[496] \"olivedrab3\"           \"olivedrab4\"           \"orange\"              \n[499] \"orange1\"              \"orange2\"              \"orange3\"             \n[502] \"orange4\"              \"orangered\"            \"orangered1\"          \n[505] \"orangered2\"           \"orangered3\"           \"orangered4\"          \n[508] \"orchid\"               \"orchid1\"              \"orchid2\"             \n[511] \"orchid3\"              \"orchid4\"              \"palegoldenrod\"       \n[514] \"palegreen\"            \"palegreen1\"           \"palegreen2\"          \n[517] \"palegreen3\"           \"palegreen4\"           \"paleturquoise\"       \n[520] \"paleturquoise1\"       \"paleturquoise2\"       \"paleturquoise3\"      \n[523] \"paleturquoise4\"       \"palevioletred\"        \"palevioletred1\"      \n[526] \"palevioletred2\"       \"palevioletred3\"       \"palevioletred4\"      \n[529] \"papayawhip\"           \"peachpuff\"            \"peachpuff1\"          \n[532] \"peachpuff2\"           \"peachpuff3\"           \"peachpuff4\"          \n[535] \"peru\"                 \"pink\"                 \"pink1\"               \n[538] \"pink2\"                \"pink3\"                \"pink4\"               \n[541] \"plum\"                 \"plum1\"                \"plum2\"               \n[544] \"plum3\"                \"plum4\"                \"powderblue\"          \n[547] \"purple\"               \"purple1\"              \"purple2\"             \n[550] \"purple3\"              \"purple4\"              \"red\"                 \n[553] \"red1\"                 \"red2\"                 \"red3\"                \n[556] \"red4\"                 \"rosybrown\"            \"rosybrown1\"          \n[559] \"rosybrown2\"           \"rosybrown3\"           \"rosybrown4\"          \n[562] \"royalblue\"            \"royalblue1\"           \"royalblue2\"          \n[565] \"royalblue3\"           \"royalblue4\"           \"saddlebrown\"         \n[568] \"salmon\"               \"salmon1\"              \"salmon2\"             \n[571] \"salmon3\"              \"salmon4\"              \"sandybrown\"          \n[574] \"seagreen\"             \"seagreen1\"            \"seagreen2\"           \n[577] \"seagreen3\"            \"seagreen4\"            \"seashell\"            \n[580] \"seashell1\"            \"seashell2\"            \"seashell3\"           \n[583] \"seashell4\"            \"sienna\"               \"sienna1\"             \n[586] \"sienna2\"              \"sienna3\"              \"sienna4\"             \n[589] \"skyblue\"              \"skyblue1\"             \"skyblue2\"            \n[592] \"skyblue3\"             \"skyblue4\"             \"slateblue\"           \n[595] \"slateblue1\"           \"slateblue2\"           \"slateblue3\"          \n[598] \"slateblue4\"           \"slategray\"            \"slategray1\"          \n[601] \"slategray2\"           \"slategray3\"           \"slategray4\"          \n[604] \"slategrey\"            \"snow\"                 \"snow1\"               \n[607] \"snow2\"                \"snow3\"                \"snow4\"               \n[610] \"springgreen\"          \"springgreen1\"         \"springgreen2\"        \n[613] \"springgreen3\"         \"springgreen4\"         \"steelblue\"           \n[616] \"steelblue1\"           \"steelblue2\"           \"steelblue3\"          \n[619] \"steelblue4\"           \"tan\"                  \"tan1\"                \n[622] \"tan2\"                 \"tan3\"                 \"tan4\"                \n[625] \"thistle\"              \"thistle1\"             \"thistle2\"            \n[628] \"thistle3\"             \"thistle4\"             \"tomato\"              \n[631] \"tomato1\"              \"tomato2\"              \"tomato3\"             \n[634] \"tomato4\"              \"turquoise\"            \"turquoise1\"          \n[637] \"turquoise2\"           \"turquoise3\"           \"turquoise4\"          \n[640] \"violet\"               \"violetred\"            \"violetred1\"          \n[643] \"violetred2\"           \"violetred3\"           \"violetred4\"          \n[646] \"wheat\"                \"wheat1\"               \"wheat2\"              \n[649] \"wheat3\"               \"wheat4\"               \"whitesmoke\"          \n[652] \"yellow\"               \"yellow1\"              \"yellow2\"             \n[655] \"yellow3\"              \"yellow4\"              \"yellowgreen\"         \n\n\nThis was just an illustration of using the color “steelblue” in a plot.\n\n```{r}\nplot(cars, col = \"steelblue\")\n```"
  },
  {
    "objectID": "class_notes/2023-03-14_multivariate-vs-univariate/index.html",
    "href": "class_notes/2023-03-14_multivariate-vs-univariate/index.html",
    "title": "Multivariate models aren’t just univariate models glued together",
    "section": "",
    "text": "Like the title says, multivariate models aren’t just univariate models glued together. In fact, there’s no straightforward way to calculate, ahead of time, what the parameters of a model y ~ x1 + x2 will be if all you have are separate univariate models y ~ x1 and y ~ x2. I’ll demonstrate that with an example modelling the bill depth of penguins, using bill length and species as predictors."
  },
  {
    "objectID": "class_notes/2023-03-14_multivariate-vs-univariate/index.html#loading-packages",
    "href": "class_notes/2023-03-14_multivariate-vs-univariate/index.html#loading-packages",
    "title": "Multivariate models aren’t just univariate models glued together",
    "section": "Loading packages",
    "text": "Loading packages\nThese are the packages we’ll use in this lesson:\n\n1library(tidyverse)\n\n2library(ggbeeswarm)\nlibrary(khroma)\nlibrary(patchwork)\ntheme_set(theme_bw())\n\n3library(palmerpenguins)\n\n4library(broom)\nlibrary(marginaleffects)\n\n\n1\n\nGeneral data processing\n\n2\n\nData visualization\n\n3\n\nThe penguins data\n\n4\n\nModel investigation"
  },
  {
    "objectID": "class_notes/2023-03-14_multivariate-vs-univariate/index.html#penguins-first-look",
    "href": "class_notes/2023-03-14_multivariate-vs-univariate/index.html#penguins-first-look",
    "title": "Multivariate models aren’t just univariate models glued together",
    "section": "Penguins, first look",
    "text": "Penguins, first look\nLet’s grab just the data columns we’re focusing on.\n\n1penguins |&gt;\n2  select(bill_depth_mm, bill_length_mm, species) |&gt;\n3  drop_na() -&gt;\n4  penguin_focus\n\n\n1\n\nThe original penguins data.\n\n2\n\nSelecting just the data columns we’re focused on.\n\n3\n\nDropping rows that have NA values.\n\n4\n\nThe new data frame."
  },
  {
    "objectID": "class_notes/2023-03-14_multivariate-vs-univariate/index.html#preparing-to-model",
    "href": "class_notes/2023-03-14_multivariate-vs-univariate/index.html#preparing-to-model",
    "title": "Multivariate models aren’t just univariate models glued together",
    "section": "Preparing to model",
    "text": "Preparing to model\nIt’s always a good idea to take a look at how each of the variables (outcome and predictor) are distributed.\n\npenguin_focus |&gt; \n1  ggplot(aes(bill_depth_mm)) +\n    stat_bin() +\n    labs(title = \"Bill Depth Histogram (outcome)\")\n\n\n1\n\nPlotting a histogram of bill_depth_mm.\n\n\n\n\n\n\n\n\npenguin_focus |&gt; \n1  ggplot(aes(bill_length_mm)) +\n    stat_bin() +\n    labs(title = \"Bill Length Histogram (predictor)\")\n\n\n1\n\nPlotting a histogram of bill_length_mm\n\n\n\n\n\n\n\n\npenguin_focus |&gt; \n1  ggplot(aes(species, fill = species))+\n    stat_count()+\n2    khroma::scale_fill_bright() +\n    labs(title = \"Species Counts (predictor)\")\n\n\n1\n\nPlotting counts of each species.\n\n2\n\nSetting a color scheme for species that we’ll reuse later.\n\n\n\n\n\n\n\nLet’s “normalize” the bill_length_mm data for modeling by z-scoring it.\n\npenguin_focus |&gt; \n1  mutate(\n2    bill_length_z = (bill_length_mm -\n                       mean(bill_length_mm)) /\n                    sd(bill_length_mm)\n  ) -&gt;\n3  penguin_focus\n\n\n1\n\nmutate() adds new columns.\n\n2\n\nWe’re subtracting the mean and dividing by the standard deviation. \\(\\frac{x-\\text{mean}(x)}{\\text{sd}(x)}\\)\n\n3\n\nAssigning the result back out.\n\n\n\n\nLet’s look at the z-scored histogram\n\npenguin_focus |&gt; \n1  ggplot(aes(bill_length_z)) +\n    stat_bin()+\n    labs(title = \"Bill Length (z-scored)\")\n\n\n1\n\nPlotting a histogram of bill_length_z."
  },
  {
    "objectID": "class_notes/2023-03-14_multivariate-vs-univariate/index.html#fitting-the-univariate-models",
    "href": "class_notes/2023-03-14_multivariate-vs-univariate/index.html#fitting-the-univariate-models",
    "title": "Multivariate models aren’t just univariate models glued together",
    "section": "Fitting the univariate models",
    "text": "Fitting the univariate models\nNormally, I’d recommend making a plot of the outcome & predictor variable before fitting a model, but just to avoid repeating too much code here, we’ll fit the models first, then plot their predicted values on the data.\n\nBill Length Model\nWe’ll start with a univariate (one variable) model, predicting bill_depth_mm with bill_length_z.\n\n1mod_len &lt;- lm(\n2   bill_depth_mm ~\n3     bill_length_z,\n4   data = penguin_focus)\n\n\n1\n\nFitting a linear model\n\n2\n\nbill_depth_mm is our outcome variable\n\n3\n\nbill_length_z is our predictor variable\n\n4\n\nUsing the penguin_focus data\n\n\n\n\nWe can look at the model parameters in a dataframe format using broom::tidy.\n\ntidy(mod_len)\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     17.2       0.104    165.   0        \n2 bill_length_z   -0.464     0.104     -4.46 0.0000112\n\n\nThe estimated effect of bill_length_z is -0.464, meaning that for every 1 standard deviation increase in bill length, we expect bill depth to decrease by -0.464 millimeters.\nWe can get the bill depth values this model predicts with the marginaleffects::predictions() function.\n\n1predictions(\n2  mod_len,\n3  newdata = datagrid(\n4    bill_length_z = c(-2, 1, 0, 1, 2)\n  )\n) |&gt; \n5  tibble()-&gt;\n  pred_len\n\n\n1\n\npredictions() is from the {marginaleffects} package.\n\n2\n\nThis is the model we want to get predictions from.\n\n3\n\nWe want to get predicted values at specific points/combinations of predictors, so we use the newdata argument. datagrid() is another function from {marginaleffects} that will create a data frame.\n\n4\n\nWe want to get predicted bill_depth_mm values for some representative bill_length_z values.\n\n5\n\nThe output of predictions() isn’t quite what we want for making a plot, so we convert it with tibble().\n\n\n\n\n\n\nThe pred_len dataframe\n\n\npred_len |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\nWe can plot these predicted values over top the original data. One thing to know is that the line we plot here is identical to what we’d get if we just added stat_smooth(method = lm) to the plot.\n\n1ggplot(\n  penguin_focus,\n  aes(bill_length_z, bill_depth_mm)\n)+\n  geom_point() +\n2  geom_line(\n    data = pred_len,\n3    aes(y = estimate),\n    color = \"steelblue\"\n  )+\n4  geom_ribbon(\n    data = pred_len,\n5    aes(\n      ymin = conf.low,\n      ymax = conf.high\n    ),\n6    alpha = 0.3\n  )\n\n\n1\n\nScatterplot of the full data.\n\n2\n\nAdding a line to represent the predicted values, which are in the pred_len dataframe.\n\n3\n\nIn pred_len, the predicted bill depth is in the estimate column.\n\n4\n\nAdding a confidence interval onto the predicted values with geom_ribbon().\n\n5\n\nThe width of geom_ribbon() is determined by the ymax and ymin aesthetics, and the high and low points of the confidence interval are in the conf.low and conf.high columns of pred_len.\n\n6\n\nSetting a transparency on the confidence interval.\n\n\n\n\n\n\n\nSo, as you can see, the line of predicted values of bill depth slopes downwards as bill length increases when we include only bill_length_z in the model.\n\n\nSpecies Model.\nNow, let’s fit a model looking at just an effect of Species. I’ve left the alphabetically first penguin species, Adelie, as the reference level.\n\n1mod_species &lt;- lm(\n2  bill_depth_mm ~\n3    species,\n4  data = penguin_focus\n)\n\n\n1\n\nWe’re fitting a linear model.\n\n2\n\nOur outcome variable is bill_depth_mm.\n\n3\n\nOur predictor variable is species.\n\n4\n\nWe’re drawing the data from penguin_focus.\n\n\n\n\nAnd again, we can look at the model parameters with broom::tidy.\n\ntidy(mod_species)\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic  p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)       18.3       0.0912   201.    0       \n2 speciesChinstrap   0.0742    0.164      0.453 6.50e- 1\n3 speciesGentoo     -3.36      0.136    -24.7   7.93e-78\n\n\nSo, the predicted bill depth for our reference level (Adelie) is 18.35 millimeters. Chinstraps are estimated to have deeper bills by 0.07mm (not significant), and Gentoos are estimated to have less deep bills by -3.36mm.\nLet’s get the predicted bill depths from our model to plot over our data.\n\n1predictions(\n2  mod_species,\n3  newdata = datagrid(\n4    species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n  )\n) |&gt; \n5  tibble()-&gt;\n  pred_species\n\n\n1\n\npredictions() from {marginaleffects}.\n\n2\n\nWe’re getting predictions from our mod_species model.\n\n3\n\nWe’re getting predicted values for this newdata generated by datagrid.\n\n4\n\nWe want predicted values for these specific species.\n\n5\n\nConversion to a dataframe.\n\n\n\n\n\n\npred_species\n\n\npred_species |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\nTo plot the original data by species, I’ll create a “beeswarm” plot, and add the predicted values on top with a “pointrange”\n\n1ggplot(\n  penguin_focus,\n  aes(\n    species,\n    bill_depth_mm,\n2    color = species\n  )\n)+\n3  geom_beeswarm(\n4    size = 0.5,\n    shape = 1\n  )+\n5  geom_pointrange(\n6    data = pred_species,\n7    aes(\n      y = estimate,\n      ymin = conf.low,\n      ymax = conf.high\n    )\n  )+ \n8  khroma::scale_color_bright()\n\n\n1\n\nSetting up the plot of the original data.\n\n2\n\nStrictly speaking, coloring the data by species is redundant for this plot, but we’re sticking to a common color scheme across all plots.\n\n3\n\nggbeeswarm::geom_beeswarm() is one way to try to deal with plotting a continuous variable against a categorical variable.\n\n4\n\nI’m making the data points small and open circles so that they’ll be visually distinct from the points we use to plot the predicted values.\n\n5\n\ngeom_pointrange() plot a point to represent the estimated value, and a “whisker” to represent the confidence interval.\n\n6\n\nOur predicted values are in pred_species.\n\n7\n\nThe predicted value is in the estimate column, and the high and low points for the confidence intervals are in conf.high and conf.low.\n\n8\n\nRe-using our consistent color scheme.\n\n\n\n\n\n\n\n\n\nUnivariate model recap.\nJust to recap what we’ve seen from our two separate univariate models:\n\nWhen we modelled bill_depth_mm with bill_length_z, we got an estimated slope of -0.464 .\nWhen we predicted bill_depth_mm with species, we estimated that Chinstraps have a bill depth 0.07mm more than Adelies, and Gentoos have a bill depth -3.36 less than Adelies."
  },
  {
    "objectID": "class_notes/2023-03-14_multivariate-vs-univariate/index.html#fitting-the-multivariate-model",
    "href": "class_notes/2023-03-14_multivariate-vs-univariate/index.html#fitting-the-multivariate-model",
    "title": "Multivariate models aren’t just univariate models glued together",
    "section": "Fitting the multivariate model",
    "text": "Fitting the multivariate model\nNow, let’s see what we get when we fit a multivatiate model, including both bill length and species in the model.\n\n1mod_len_species &lt;- lm(\n2  bill_depth_mm ~\n3    bill_length_z +\n    species,\n4  data = penguin_focus\n)\n\n\n1\n\nWe’re fitting a linear model\n\n2\n\nOur outcome variable is bill_depth_mm\n\n3\n\nOur predictor variables are bill_length_z and species\n\n4\n\nWe’re drawing the data from the penguin_focus dataframe.\n\n\n\n\nLet’s look at our model parameters now!\n\ntidy(mod_len_species)\n\n# A tibble: 4 × 5\n  term             estimate std.error statistic   p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         19.4     0.119     163.   2   e-323\n2 bill_length_z        1.09    0.0955     11.4  8.66e- 26\n3 speciesChinstrap    -1.93    0.224      -8.62 2.55e- 16\n4 speciesGentoo       -5.11    0.191     -26.7  3.65e- 85\n\n\nThe estimated effects of bill length and species in this multivariate model are really different from each separate univariate model. Let’s look at them right next to each other, comparing the multivariate model to the “glued together” univariate model estimates.\n\n\n\n\n\n\n\n\n\nterm\nmultivariate\nglued togther univariate\n\n\n\n\nbill_length_z\n1.09\n−0.46\n\n\nspeciesChinstrap\n−1.93\n0.07\n\n\nspeciesGentoo\n−5.11\n−3.36\n\n\n\n\n\n\n\n\nThe effect of bill_length_z has gone from negative in the univariate model to positive in the multivariate model, meaning we’re now predicting that as bill length gets longer, bill depth gets deeper.\nChinstraps have gone from having a predicted bill depth that’s just about the same as Adelies in the univariate model to a predicted shorter bill depth.\nGentoos predicted to have a less deep bill than Adelies in both models, but the difference is predicted to be larger in the multivariate model.\n\nWhy are the estimates so different between the multivariate and univariate models? Let’s get predicted values to plot over the data.\n\n1predictions(\n2  mod_len_species,\n3  newdata = datagrid(\n4    bill_length_z = c(-2, 1, 0, 1, 2),\n    species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n  )\n) |&gt; \n5  tibble() -&gt;\n  pred_len_species\n\n\n1\n\npredictions() from {marginaleffects}.\n\n2\n\nWe’re getting predictions from our multivariate mod_len_species model.\n\n3\n\nWe’re going to get predictions for the newdata generated by datagrid(). This time, because we’re creating two columns, datagrid() is going to create a row for every combination of values in bill_length_z and species.\n\n4\n\nWe want predictions for representative values of bill_length_z and species.\n\n5\n\nConversion to a dataframe.\n\n\n\n\n\n\npred_len_species\n\n\npred_len_species |&gt; \n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\nLet’s first plot these predicted values over the bill length x bill depth scatterplot.\n\n1ggplot(\n  penguin_focus,\n  aes(\n    bill_length_z,\n    bill_depth_mm,\n    color = species\n    )\n)+\n  geom_point()+\n2  geom_line(\n    data = pred_len_species,\n    aes(y = estimate)\n  )+\n3  geom_ribbon(\n    data = pred_len_species,\n    aes(\n      ymin = conf.low,\n      ymax = conf.high,\n      fill = species\n    ),\n4    color = NA,\n    alpha = 0.3\n  ) +\n5  scale_color_bright()+\n  scale_fill_bright()\n\n\n1\n\nSetting up the scatter plot, this time coloring points according to penguin species.\n\n2\n\nAdding on the predicted values from pred_len_species.\n\n3\n\nAdding on confidence intervals.\n\n4\n\nThe plot looks nicer with these settings.\n\n5\n\nUsing our color scheme for both the color of the points & lines and the fill of the ribbons.\n\n\n\n\n\n\n\nWhat we can see now is that when looking at the data overall, the relationship between bill length and bill depth might be negative, but when looking within each species, the relationship is positive. For Adelies, penguins with longer bills have deeper bills, and the same goes for Chinstraps and Gentoos.\nThe original slope in the univariate model was negative because each species has a different baseline bill depth and different ranges of bill lengths. When fitting a univariate model that didn’t take species into account, this results in a negative slope! This mismatch between the effect within groups vs combining groups is known as Simpson’s Paradox."
  },
  {
    "objectID": "class_notes/2023-03-14_multivariate-vs-univariate/index.html#remaining-questions",
    "href": "class_notes/2023-03-14_multivariate-vs-univariate/index.html#remaining-questions",
    "title": "Multivariate models aren’t just univariate models glued together",
    "section": "Remaining Questions:",
    "text": "Remaining Questions:\n\nCould we have predicted what the slopes were going to be in the multivariate model?\nWhen we were learning about centering and scaling predictors for univariate models, it was possible to calculate what the new parameters were going to be after (linearly) transforming them. Could we have mathematically calculated what the slopes were going to be in the multivariate model based on the two univariate models?\nNo. The negative slope in the bill_depth_mm ~ bill_length_z, for example, was estimated with information about species missing (i.e. bill_depth_mm’s relationship to species and bill_length_z’s relationship to species).\n\n\nDid the univariate models miscalculate the relationship between bill length, species, and bill depth?\nNo, not really. As it turns out, the univariate models aren’t very good models for predicting bill depth. If we compare their model fits as well as \\(R^2\\) and goodness of fit heuristics, AIC and BIC, the multivariate model is preferred.\n\n\n\n\n\n\n\n\n\nmodel\nr.squared\nAIC\nBIC\n\n\n\n\n~len\n0.06\n1,422\n1,433\n\n\n~species\n0.68\n1,054\n1,069\n\n\n~len + species\n0.77\n944\n963\n\n\n\n\n\n\n\nBut, just because the univariate models aren’t ideal, that doesn’t mean they miscalculated anything. When trying to estimate the effect of bill length on bill depth in the absence of species information, the slope is negative.\n\n\nHow do I visualize fitted multivariate models?\nOne important takeaway is that in order to visualize the results of a multivariate model, you can’t just plot the fit of a univariate model. This plot doesn’t show a line with a slope of 1.09.\n\nggplot(\n  penguin_focus, \n  aes(\n    bill_length_z, \n    bill_depth_mm\n    )\n)+\n  geom_point()+\n  stat_smooth(method = lm)\n\n\n\n\nInstead, you need to get predicted values from the multivariate model. The code above used marginaleffects::predictions() to do that, but there are actually lots of ways to go about it. For example, here’s some base R code to do it.\n\npred &lt;- expand.grid(\n  bill_length_z = c(-2, 1, 0, 1, 2),\n  species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\n)\n\npred$estimate &lt;- predict(mod_len_species, newdata = pred)\n\n\nggplot(\n  penguin_focus,\n  aes(\n    bill_length_z,\n    bill_depth_mm,\n    color = species\n  )\n)+\n  geom_point() +\n  geom_line(\n    data = pred,\n    aes(y = estimate)\n  )+\n  scale_color_bright()"
  },
  {
    "objectID": "class_notes/2023-01-12_Quarto/index.html",
    "href": "class_notes/2023-01-12_Quarto/index.html",
    "title": "Plan for Onboarding Day",
    "section": "",
    "text": "Follow Plan here: Onboarding"
  },
  {
    "objectID": "class_notes/2023-01-12_Quarto/index.html#configure-rstudio-to-communicate-with-github",
    "href": "class_notes/2023-01-12_Quarto/index.html#configure-rstudio-to-communicate-with-github",
    "title": "Plan for Onboarding Day",
    "section": "",
    "text": "Follow Plan here: Onboarding"
  },
  {
    "objectID": "class_notes/2023-01-12_Quarto/index.html#rstudio-project-creation",
    "href": "class_notes/2023-01-12_Quarto/index.html#rstudio-project-creation",
    "title": "Plan for Onboarding Day",
    "section": "RStudio Project Creation",
    "text": "RStudio Project Creation\n\nGeneral RStudio Project Creation\nRStudio Project creation from Github Repository"
  },
  {
    "objectID": "class_notes/2023-01-12_Quarto/index.html#quarto-intro",
    "href": "class_notes/2023-01-12_Quarto/index.html#quarto-intro",
    "title": "Plan for Onboarding Day",
    "section": "Quarto Intro",
    "text": "Quarto Intro\n\n.qmd editing in the visual editor\n\nOpen Week 1 index.qmd.\n\nbasic formatting\n\nheaders\nitalics\nmonospace\nlinks\n\n\n\nyaml header\n\nLots of options: https://quarto.org/docs/reference/formats/html.html\ntitle:\nauthor:\ndate:\ntoc:\ndraft:\n\n\nthe _metadata file\nthe _quarto file\nCode chunks\n\n\n1+1\n\n[1] 2\n\n\nRender\nUnder the Build tab, clicking “Render Website” will render the whole website as a preview. The actual html pages etc are created in the _site folder by default."
  },
  {
    "objectID": "class_notes/2023-01-12_Quarto/index.html#visual-mode-goodies",
    "href": "class_notes/2023-01-12_Quarto/index.html#visual-mode-goodies",
    "title": "Plan for Onboarding Day",
    "section": "Visual Mode Goodies",
    "text": "Visual Mode Goodies\n\n\nForward slash / to bring up insert palette at the start of a new paragraph, or ⌘+/ or CMD+/ for inline options.\n\ne.g. images, tables, footnotes"
  },
  {
    "objectID": "class_notes/2023-01-12_Quarto/index.html#tricking-out-rstudio",
    "href": "class_notes/2023-01-12_Quarto/index.html#tricking-out-rstudio",
    "title": "Plan for Onboarding Day",
    "section": "Tricking out RStudio",
    "text": "Tricking out RStudio\nUnder Tools&gt;Global Options there are a few things you might really want to customize, including\n\nAppearance: Will change the appearance and color themes of RStudio\nRMarkdown: Under the Citations tab, you can connect RStudio up to your Zotero"
  },
  {
    "objectID": "class_notes/2023-01-31_tidy-plot/index.html",
    "href": "class_notes/2023-01-31_tidy-plot/index.html",
    "title": "Tidying and Plotting",
    "section": "",
    "text": "library(tidyverse)\nlibrary(nasapower)\n\nGrabbing monthly temperature averages for Lexington. Untidy data.\n\nlex_temp &lt;- \n  get_power(\n    community = \"ag\",\n    temporal_api = \"monthly\",\n    pars = \"T2M\",\n    dates = c(\"1985-01-01\", \"2021-12-31\"),\n    lonlat = c(-84.501640,  38.047989)\n  )\n\nUntidy because each row has 12 different observations (1 for each month). Column names JAN through DEC should be variables.\n\nlex_temp\n\n\n\n\n\n  \n\n\n\nPivoting from wide to long\n\nlex_temp |&gt;\n  pivot_longer(\n    # which columns should go long?\n    cols = JAN:DEC,\n    # where should the column names go?\n    names_to = \"month\",\n    # where shoild the column values go?\n    values_to = \"temp\"\n  )"
  },
  {
    "objectID": "class_notes/2023-01-31_tidy-plot/index.html#recap",
    "href": "class_notes/2023-01-31_tidy-plot/index.html#recap",
    "title": "Tidying and Plotting",
    "section": "",
    "text": "library(tidyverse)\nlibrary(nasapower)\n\nGrabbing monthly temperature averages for Lexington. Untidy data.\n\nlex_temp &lt;- \n  get_power(\n    community = \"ag\",\n    temporal_api = \"monthly\",\n    pars = \"T2M\",\n    dates = c(\"1985-01-01\", \"2021-12-31\"),\n    lonlat = c(-84.501640,  38.047989)\n  )\n\nUntidy because each row has 12 different observations (1 for each month). Column names JAN through DEC should be variables.\n\nlex_temp\n\n\n\n\n\n  \n\n\n\nPivoting from wide to long\n\nlex_temp |&gt;\n  pivot_longer(\n    # which columns should go long?\n    cols = JAN:DEC,\n    # where should the column names go?\n    names_to = \"month\",\n    # where shoild the column values go?\n    values_to = \"temp\"\n  )"
  },
  {
    "objectID": "class_notes/2023-01-31_tidy-plot/index.html#getting-untidy-data",
    "href": "class_notes/2023-01-31_tidy-plot/index.html#getting-untidy-data",
    "title": "Tidying and Plotting",
    "section": "Getting untidy data",
    "text": "Getting untidy data\nExample untidy (linguistic!) data can be found in Joseph Casillas’ package on github.\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"jvcasillas/untidydata\")\n\n\nlibrary(untidydata)\n\nVowel formant estimates for spanish vowels. The data column label follows good file naming protocol, but poor data column protocol. Three different variables smushed together into one:\n\nspeaker id\nspeaker gender\nvowel class\n\n\nspanish_vowels\n\n\n\n\n\n  \n\n\n\nThese three columns can be separated out with the tidyr::separate() function.\n\ntidy_vowels &lt;-\n  spanish_vowels |&gt;\n  separate(\n    # which column to separate\n    col = label,\n    # how to separate them\n    sep = \"-\",\n    # what to call the new columns\n    into = c(\"id\", \"gender\", \"vowel\")\n  )\n\n\ntidy_vowels"
  },
  {
    "objectID": "class_notes/2023-01-31_tidy-plot/index.html#plotting",
    "href": "class_notes/2023-01-31_tidy-plot/index.html#plotting",
    "title": "Tidying and Plotting",
    "section": "Plotting",
    "text": "Plotting\nMaking a ggplot vowel plot from tidy_vowels.\nggplot2 resources\n\nhttps://r4ds.hadley.nz/data-visualize.html\nhttps://ggplot2-book.org/\n\nThese plots are built by adding “layers”\nData Layer\n\nThe aes() function is used to map data variables to plot aesthetics.\n\n\ntidy_vowels |&gt; \n  ggplot(aes(x = f2, y = f1))\n\n\n\n\nGeometry layer\n“geometries” are the visual components of plots.\n\ntidy_vowels |&gt;\n  ggplot(aes(x = f2, y = f1)) +\n    geom_point()\n\n\n\n\nWe can set certain visual components of geometries.\n\ntidy_vowels |&gt;\n  ggplot(aes(x = f2, y = f1)) +\n    geom_point(\n      color = \"#BE3455\",\n      size = 4,\n      # alpha is transparency\n      alpha = 0.6,\n      shape = \"square\"\n    )\n\n\n\n\nWe can also map data to the visual components.\n\ntidy_vowels |&gt;\n  ggplot(\n    aes(\n      x = f2, \n      y = f1,\n      color = vowel\n    )\n  ) +\n    geom_point()\n\n\n\n\nStatistic layers\nWe can add “statistic” layers to plots as well.\n\ntidy_vowels |&gt;\n  ggplot(\n    aes(\n      x = f2, \n      y = f1,\n      color = vowel\n    )\n  ) +\n    geom_point()+\n    # Doesn't really make sense\n    stat_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\ntidy_vowels |&gt;\n  ggplot(\n    aes(\n      x = f2, \n      y = f1,\n      color = vowel\n    )\n  ) +\n    geom_point()+\n    stat_ellipse()\n\n\n\n\nScale layers\nWe can adjust the “scales” of the spatial axes and other aesthetic mappings with scale layers.\n\n# this might need installing\nlibrary(khroma)\n\n\ntidy_vowels |&gt;\n  ggplot(\n    aes(\n      x = f2, \n      y = f1,\n      color = vowel\n    )\n  ) +\n    geom_point()+\n    stat_ellipse()+\n    # reverse x and y\n    scale_y_continuous(trans = \"reverse\")+\n    scale_x_continuous(trans = \"reverse\")+\n    scale_color_vibrant()\n\n\n\n\nTitles\nThe ggplot2::labs() layer will do you.\n\ntidy_vowels |&gt;\n  ggplot(\n    aes(\n      x = f2, \n      y = f1,\n      color = vowel\n    )\n  ) +\n    geom_point()+\n    stat_ellipse()+\n    # reverse x and y\n    scale_y_continuous(trans = \"reverse\")+\n    scale_x_continuous(trans = \"reverse\")+\n    scale_color_vibrant()+\n    labs(title = \"vowels\",\n         x = \"F2 (hz)\",\n         y = \"F1 (hz)\",\n         color = \"vowel\\nclass\")\n\n\n\n\nFaceting\nYou can make small multiples with ggplot2::facet_wrap() or ggplot2::facet_grid().\n\ntidy_vowels |&gt;\n  ggplot(\n    aes(\n      x = f2, \n      y = f1,\n      color = vowel\n    )\n  ) +\n    geom_point()+\n    stat_ellipse()+\n    # reverse x and y\n    scale_y_continuous(trans = \"reverse\")+\n    scale_x_continuous(trans = \"reverse\")+\n    scale_color_vibrant()+\n    labs(title = \"vowels\",\n         x = \"F2 (hz)\",\n         y = \"F1 (hz)\",\n         color = \"vowel\\nclass\")+\n    facet_wrap(~gender)\n\n\n\n\nTheming\nggplot2 has a number of built in themes\n\ntidy_vowels |&gt;\n  ggplot(\n    aes(\n      x = f2, \n      y = f1,\n      color = vowel\n    )\n  ) +\n    geom_point()+\n    stat_ellipse()+\n    # reverse x and y\n    scale_y_continuous(trans = \"reverse\")+\n    scale_x_continuous(trans = \"reverse\")+\n    scale_color_vibrant()+\n    labs(title = \"vowels\",\n         x = \"F2 (hz)\",\n         y = \"F1 (hz)\",\n         color = \"vowel\\nclass\")+\n    facet_wrap(~gender) +\n    theme_minimal()\n\n\n\n\nYou can get additional fine-grained control with ggplot2::theme()\n\ntidy_vowels |&gt;\n  ggplot(\n    aes(\n      x = f2, \n      y = f1,\n      color = vowel\n    )\n  ) +\n    geom_point()+\n    stat_ellipse()+\n    # reverse x and y\n    scale_y_continuous(trans = \"reverse\")+\n    scale_x_continuous(trans = \"reverse\")+\n    scale_color_vibrant()+\n    labs(title = \"vowels\",\n         x = \"F2 (hz)\",\n         y = \"F1 (hz)\",\n         color = \"vowel\\nclass\")+\n    facet_wrap(~gender) +\n    theme_minimal() +\n    theme(\n      legend.position = \"top\",\n      aspect.ratio = 1\n      )\n\n\n\n\nCombining with tidy workflows\nTo label each vowel cluster with its vowel class, we need to calculate the F1 and F2 means for each vowel for each gender.\n\nvowel_means &lt;- \n  tidy_vowels |&gt;\n  group_by(vowel, gender) |&gt;\n  summarise(f1= mean(f1), f2 = mean(f2))\n\n`summarise()` has grouped output by 'vowel'. You can override using the\n`.groups` argument.\n\n\nNow add a geom_label() layer on after the geom_point() layer.\n\ntidy_vowels |&gt;\n  ggplot(\n    aes(\n      x = f2, \n      y = f1,\n      color = vowel\n    )\n  ) +\n    geom_point()+\n    geom_label(\n      data = vowel_means,\n      aes(label = vowel)\n    )+\n    stat_ellipse()+\n    # reverse x and y\n    scale_y_continuous(trans = \"reverse\")+\n    scale_x_continuous(trans = \"reverse\")+\n    scale_color_vibrant()+\n    labs(title = \"vowels\",\n         x = \"F2 (hz)\",\n         y = \"F1 (hz)\",\n         color = \"vowel\\nclass\")+\n    facet_wrap(~gender) +\n    theme_minimal() +\n    theme(\n      legend.position = \"top\",\n      aspect.ratio = 1\n      )\n\n\n\n\nStrictly speaking, the legend isn’t necessary anymore with the direct labels. I’ll drop it with the guides() layer. I’ve placed it after the scale_ layers, just for code clarity.\n\ntidy_vowels |&gt;\n  ggplot(\n    aes(\n      x = f2, \n      y = f1,\n      color = vowel\n    )\n  ) +\n    geom_point()+\n    geom_label(\n      data = vowel_means,\n      aes(label = vowel)\n    )+\n    stat_ellipse()+\n    # reverse x and y\n    scale_y_continuous(trans = \"reverse\")+\n    scale_x_continuous(trans = \"reverse\")+\n    scale_color_vibrant()+\n    guides(color = \"none\")+\n    labs(title = \"vowels\",\n         x = \"F2 (hz)\",\n         y = \"F1 (hz)\",\n         color = \"vowel\\nclass\")+\n    facet_wrap(~gender) +\n    theme_minimal() +\n    theme(\n      aspect.ratio = 1\n      )"
  },
  {
    "objectID": "class_notes/2023-01-23_tidyverse/index.html",
    "href": "class_notes/2023-01-23_tidyverse/index.html",
    "title": "Starting with the tidyverse",
    "section": "",
    "text": "Let’s start by loading the tidyverse.\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.0 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "class_notes/2023-01-23_tidyverse/index.html#tidyverse-functions-as-verbs",
    "href": "class_notes/2023-01-23_tidyverse/index.html#tidyverse-functions-as-verbs",
    "title": "Starting with the tidyverse",
    "section": "Tidyverse functions as verbs",
    "text": "Tidyverse functions as verbs\nMost tidyverse functions are written to be verbs, taje a data frame as their first argument, and also return a data frame.\n\n# a data frame\nmtcars &lt;- as_tibble(mtcars)\nmtcars\n\n# A tibble: 32 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# … with 22 more rows\n\n\n\n# filter the dataframe to \n# only the rows with cyl==6\nfilter(mtcars, cyl == 6)\n\n# A tibble: 7 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n3  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n4  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n5  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n6  17.8     6  168.   123  3.92  3.44  18.9     1     0     4     4\n7  19.7     6  145    175  3.62  2.77  15.5     0     1     5     6\n\n\n\n# count how many rows \n# have these values of gears\ncount(mtcars, gear)\n\n# A tibble: 3 × 2\n   gear     n\n  &lt;dbl&gt; &lt;int&gt;\n1     3    15\n2     4    12\n3     5     5"
  },
  {
    "objectID": "class_notes/2023-01-23_tidyverse/index.html#piping",
    "href": "class_notes/2023-01-23_tidyverse/index.html#piping",
    "title": "Starting with the tidyverse",
    "section": "Piping",
    "text": "Piping\nSince tidyverse functions take data frames as input, and produce data frames as output, you might want to combine them.\nWhat are the counts of gear for cars with cyl==6?\n\ncount(\n  filter(\n    mtcars, \n    cyl == 6\n    ), \n  gear\n  )\n\n# A tibble: 3 × 2\n   gear     n\n  &lt;dbl&gt; &lt;int&gt;\n1     3     2\n2     4     4\n3     5     1\n\n\nA problem here is that you have to write, and read your functions inside out. Wouldn’t it be great if we could write code that looks like:\n\nFirst take the mtcars data, and then filter it by cyl==6, then get the count of gears.\n\nThat’s where the pipe |&gt; comes in. The pipe takes everything to its left, and inserts it as the first argument to the function on its right.\n\n# this\nmtcars |&gt; filter(cyl == 6)\n\n# is equivalent to this\nfilter(mtcars, cyl == 6)\n\nThis lets us chain tidyverse verbs together.\n\nmtcars |&gt; \n  filter(cyl == 6) |&gt; \n  count(gear)\n\n# A tibble: 3 × 2\n   gear     n\n  &lt;dbl&gt; &lt;int&gt;\n1     3     2\n2     4     4\n3     5     1\n\n\n\n\n\n\n\n\nWork it out\n\n\n\nThe dataframe starwars contains demographic and personal data for many characters from the Star Wars universe. Using dplyr verbs like\n\nfilter()\ncount()\narrange()\nslice()\nselect()\n\n\nFind out which planet is the most common homeworld for humans.\nFind out who was the tallest Droid."
  },
  {
    "objectID": "class_notes/2023-01-23_tidyverse/index.html#grouping-and-summarizing",
    "href": "class_notes/2023-01-23_tidyverse/index.html#grouping-and-summarizing",
    "title": "Starting with the tidyverse",
    "section": "Grouping and summarizing",
    "text": "Grouping and summarizing\nTo find out average horsepower across all of the cars in mtcars, we can use summarise()\n\nmtcars |&gt; \n  summarise(hp = mean(hp))\n\n# A tibble: 1 × 1\n     hp\n  &lt;dbl&gt;\n1  147.\n\n\nIf we wanted to find out the average horsepower by the number of cylinders, we can group_by() and then summarise().\n\nmtcars |&gt; \n  group_by(cyl) |&gt; \n  summarise(hp = mean(hp))\n\n# A tibble: 3 × 2\n    cyl    hp\n  &lt;dbl&gt; &lt;dbl&gt;\n1     4  82.6\n2     6 122. \n3     8 209."
  },
  {
    "objectID": "class_notes/2023-01-23_tidyverse/index.html#mutating",
    "href": "class_notes/2023-01-23_tidyverse/index.html#mutating",
    "title": "Starting with the tidyverse",
    "section": "Mutating",
    "text": "Mutating\nTo add new columns to a data frame, we can use mutate(). Inside of mutate, we can make reference to any column in the dataframe.\n\n## horsepower by cylinder?\nmtcars |&gt; \n  mutate(hp_by_cyl = hp/cyl)\n\n# A tibble: 32 × 12\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb hp_by_cyl\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4      18.3\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4      18.3\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1      23.2\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1      18.3\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2      21.9\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1      17.5\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4      30.6\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2      15.5\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2      23.8\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4      20.5\n# … with 22 more rows\n\n\n\n\n\n\n\n\nWork it out\n\n\n\nThis will load all tokens of “uh” and “um” from the Philadelphia Neighborhood Corpus.\n\num &lt;- read_tsv(\"https://bit.ly/3JdeSbx\")\n\nRows: 26060 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): word, next_seg, idstring\ndbl (11): start_time, end_time, vowel_start, vowel_end, nasal_start, nasal_e...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe column word codes whether it was “um” or “uh” or some combo that was spoken. Other important columns are\n\nstart_time, end_time: the start and end times for the whole word\nvowel_start, vowel_end: The start and end time of the vowel in the word.\nnasal_start, nasal_end: The start and end times of the nasal, for the word UM.\nnext_seg: the transcription of the following segment. \"sp\" means “pause”\nnext_seg_start, next_seg_end the start and end times of the following segment\n\n\num\n\n# A tibble: 26,060 × 14\n   word  start…¹ end_t…² vowel…³ vowel…⁴ nasal…⁵ nasal…⁶ next_…⁷ next_…⁸ next_…⁹\n   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 UH       24.4    24.7    24.4    24.7    NA      NA   S          24.7    24.9\n 2 UH       35.0    35.2    35.0    35.2    NA      NA   F          35.2    35.4\n 3 UM       37.9    38.3    37.9    38.1    38.1    38.3 sp         38.3    38.4\n 4 UH       44.5    44.7    44.5    44.7    NA      NA   DH         44.7    44.7\n 5 UH       57.6    57.8    57.6    57.8    NA      NA   AY1        57.8    57.9\n 6 UH       62.3    62.5    62.3    62.5    NA      NA   sp         62.5    63.0\n 7 UH       73.9    74.2    73.9    74.2    NA      NA   sp         74.2    75.0\n 8 UH       75.1    75.4    75.1    75.4    NA      NA   sp         75.4    75.7\n 9 UM       81.6    82.0    81.6    81.8    81.8    82.0 sp         82.0    84.0\n10 UH       92.6    92.9    92.6    92.9    NA      NA   sp         92.9    93.4\n# … with 26,050 more rows, 4 more variables: chunk_start &lt;dbl&gt;,\n#   chunk_end &lt;dbl&gt;, nwords &lt;dbl&gt;, idstring &lt;chr&gt;, and abbreviated variable\n#   names ¹​start_time, ²​end_time, ³​vowel_start, ⁴​vowel_end, ⁵​nasal_start,\n#   ⁶​nasal_end, ⁷​next_seg, ⁸​next_seg_start, ⁹​next_seg_end\n\n\nUsing dplyr verbs like\n\nmutate()\ngroup_by()\nsummarise()\n\n\nFigure out the average duration of the vowel for each kind of word.\nFigure out the average duration of the vowel for each kind of word when the following segment is a pause versus when it isn’t."
  },
  {
    "objectID": "class_notes/2023-01-17_osf-data/index.html",
    "href": "class_notes/2023-01-17_osf-data/index.html",
    "title": "Downloading OSF Data",
    "section": "",
    "text": "The data for winter’s Statistics for Linguistics textbook is all available on the Open Science Framework at https://osf.io/34mq9/ You can download the files as you need them and upload them into your blog files, or you can download the whole thing at once with this quarto notebook and the osfr package.\nI’m effectively just running through their sample documentation from the about page."
  },
  {
    "objectID": "class_notes/2023-01-17_osf-data/index.html#step-1---install-osfr-and-load-it",
    "href": "class_notes/2023-01-17_osf-data/index.html#step-1---install-osfr-and-load-it",
    "title": "Downloading OSF Data",
    "section": "Step 1 - Install osfr and load it",
    "text": "Step 1 - Install osfr and load it\nThis will check to see of osfr is installed. If it is, it will load it. If not, it will install it, then load it. This may start printing out scary looking things like g++ -std=gnu++14 -I\"/usr/share/R/include\" and so forth. That is ok.\n\nosfr_exists &lt;- require(\"osfr\")\n\nLoading required package: osfr\n\nif(!osfr_exists){\n  install.packages(\"osfr\")\n  library(osfr)\n}"
  },
  {
    "objectID": "class_notes/2023-01-17_osf-data/index.html#step-2---download-the-data",
    "href": "class_notes/2023-01-17_osf-data/index.html#step-2---download-the-data",
    "title": "Downloading OSF Data",
    "section": "Step 2 - Download the data",
    "text": "Step 2 - Download the data\nThe little string of letters and numbers comes from the osf link. I’ve included some if()s in there to be defensive, just in case you re-run the code so you won’t accidentally delete or overwrite anything.\n\n# I'm just being defensive here,\n# in case you've run the script before.\ndata_exists &lt;- dir.exists(\"data\")\nif(!data_exists){\n  dir.create(path = \"data\")\n}\n\n\ndata_files &lt;- list.files(\"data\")\nif(length(data_files) != 17){\n  osf_retrieve_node('34mq9') |&gt; \n    osf_ls_files(\n      \"materials/data\", \n      n_max = Inf\n    ) |&gt; \n    osf_download(\n      path = \"data\",\n      conflicts = \"skip\"\n    )\n}"
  },
  {
    "objectID": "class_notes/2023-01-17_osf-data/index.html#step-3---load-the-data-into-your-blog-post.",
    "href": "class_notes/2023-01-17_osf-data/index.html#step-3---load-the-data-into-your-blog-post.",
    "title": "Downloading OSF Data",
    "section": "Step 3 - Load the data into your blog post.",
    "text": "Step 3 - Load the data into your blog post.\nThere are two ways to go about loading the data into a quarto notebook for your work-through blog post.\nWay 1\nI’ve assumed you’ve run the download_data.qmd code from the main workthrough blog directory. So, from any given blog post in posts/XX_chapter/index.qmd file, you’d need to type\n\nnettle_df &lt;- read.csv(\"../../data/nettle_1999_climate.csv\")\n\nWay 2\nIn the file browser pane in RStudio, if you navigate to the data directory, click on the “⚙️ More” drop down menu. Then select Copy Folder Path to Clipboard. For me, this copies ~/work_through_blog/data to the clipboard. You can then use this inside read.csv() like so:\n\nnettle_df &lt;- read.csv(\"~/work_through_blog/data/nettle_1999_climate.csv\")"
  },
  {
    "objectID": "class_notes/2023-01-17_RLang/index.html",
    "href": "class_notes/2023-01-17_RLang/index.html",
    "title": "Basics of R Syntax",
    "section": "",
    "text": "To run R code in a Quarto notebook, you need to insert a “code chunk”. In visual editor mode, you can do that by typing the forward slash (/) and start typing in “R Code Chunk”. In the source editor mode, you have to have a line with ```{r} (three “backticks” followed by “r” in curly braces), then a few blank lines followed by another ```\n\n```{r}\n1+1\n```\n\n[1] 2\n\n\nTo actually run the code, you can either click on the green play button, or press the appropriate hotkey for your system (COMMAND+RETURN on a mac, CTRL+ENTER on windows)."
  },
  {
    "objectID": "class_notes/2023-01-17_RLang/index.html#running-r-code-in-a-quarto-notebook",
    "href": "class_notes/2023-01-17_RLang/index.html#running-r-code-in-a-quarto-notebook",
    "title": "Basics of R Syntax",
    "section": "",
    "text": "To run R code in a Quarto notebook, you need to insert a “code chunk”. In visual editor mode, you can do that by typing the forward slash (/) and start typing in “R Code Chunk”. In the source editor mode, you have to have a line with ```{r} (three “backticks” followed by “r” in curly braces), then a few blank lines followed by another ```\n\n```{r}\n1+1\n```\n\n[1] 2\n\n\nTo actually run the code, you can either click on the green play button, or press the appropriate hotkey for your system (COMMAND+RETURN on a mac, CTRL+ENTER on windows)."
  },
  {
    "objectID": "class_notes/2023-01-17_RLang/index.html#mathematical-operations",
    "href": "class_notes/2023-01-17_RLang/index.html#mathematical-operations",
    "title": "Basics of R Syntax",
    "section": "Mathematical Operations",
    "text": "Mathematical Operations\nAddition\n\n1 + 4\n\n[1] 5\n\n\nSubtraction\n\n1 - 4\n\n[1] -3\n\n\nMultiplication\n\n5 * 4\n\n[1] 20\n\n\nDivision\n\n5 / 4\n\n[1] 1.25\n\n\nExponentiation\n\n5 ^ 4\n\n[1] 625\n\n\nOrders of Operation\nHonestly, instead of gambling on how R may or may not interpret PEMDAS, just add parentheses ( ) around every operation in the order you want it to happen.\n\n(5 ^ (2 * 2)) / 6\n\n[1] 104.1667"
  },
  {
    "objectID": "class_notes/2023-01-17_RLang/index.html#assignment",
    "href": "class_notes/2023-01-17_RLang/index.html#assignment",
    "title": "Basics of R Syntax",
    "section": "Assignment",
    "text": "Assignment\nTo assign values to a variable in R, you can use either &lt;- or -&gt;. Most style guides shun -&gt;, but I actually wind up using it a lot.\n\nmy_variable &lt;- 4 * 5\nprint(my_variable)\n\n[1] 20\n\n\n\nmy_variable / 2\n\n[1] 10"
  },
  {
    "objectID": "class_notes/2023-01-17_RLang/index.html#data-types",
    "href": "class_notes/2023-01-17_RLang/index.html#data-types",
    "title": "Basics of R Syntax",
    "section": "Data Types",
    "text": "Data Types\nNumeric\nWhen using a number in R, we can only use digits and dots (.). If we try to enter “one hundred thousand” with a comma separator, we’ll get an error.\n\nbig_number &lt;- 100,000\n\nError: &lt;text&gt;:1:18: unexpected ','\n1: big_number &lt;- 100,\n                     ^\n\n\nWe also can’t use any percent signs (%) or currency symbols ($, £, €)\nCharacters\nWhen we type in text without any quotes, R will assume it’s a variable or function that’s already been defined and go looking for it.\n\nlarge &lt;- 100000\nlarge\n\n[1] 1e+05\n\n\nIf the variable hasn’t been created already, we’ll get an error.\n\nsmall\n\nError in eval(expr, envir, enclos): object 'small' not found\n\n\nIf we enter text inside of quotation marks, either single quotes ' or double quotes \", R will instead treat the text as a value that we could, for example, assign to a variable, or just print out.\n\n\"small\"\n\n[1] \"small\"\n\n\n\ntiny_synonym &lt;- \"small\"\ntiny_synonym\n\n[1] \"small\"\n\n\n\n\n\n\n\n\nCommon Error\n\n\n\nYou will often get confused about this and get the Error: object '' not found message. Even if you do this for 15 years, you will still sometimes enter plain text when you meant to put it in quotes, and put text in quotes you meant to enter without. It’s always annoying, but doesn’t mean you’re bad at doing this.\n\n\nLogical\nThere are two specialized values that you could call “True/False” or “Logical” or “Boolean” values\n\n# fullnames\nTRUE\n\n[1] TRUE\n\nFALSE\n\n[1] FALSE\n\n\n\n# Short Forms\nT\n\n[1] TRUE\n\nF\n\n[1] FALSE\n\n\nThese are often created using logical comparisons\n\nlarge  &lt;- 100000\nmedium &lt;-    600\n\nlarge &lt; medium\n\n[1] FALSE\n\n\n\nshort_word &lt;- \"to\"\n\nnchar(short_word) == 2\n\n[1] TRUE\n\n\nNA\nWhen you have a missing value, that’s given a special NA value.\n\nnumbers &lt;- c(1, NA, 5)\nnumbers\n\n[1]  1 NA  5"
  },
  {
    "objectID": "class_notes/2023-01-17_RLang/index.html#vectors",
    "href": "class_notes/2023-01-17_RLang/index.html#vectors",
    "title": "Basics of R Syntax",
    "section": "Vectors",
    "text": "Vectors\nVectors are basically 1 dimensional lists of values.1 You can have numeric, character or logical vectors in R, but you can’t mix types. One way to create vectors is with the c() (for concatenate) function. There needs to be a comma , between every value that you add to a vector.\n\ndigital_words &lt;- c(\n  \"-dle\",\n  \"BFFR\",\n  \"chief twit\",\n  \"chronically online\",\n  \"crypto rug pull\",\n  \"touch grass\",\n  \"-verse\"\n)\nprint(digital_words)\n\n[1] \"-dle\"               \"BFFR\"               \"chief twit\"        \n[4] \"chronically online\" \"crypto rug pull\"    \"touch grass\"       \n[7] \"-verse\"            \n\n\n\ndigital_word_votes &lt;- c(\n  84,\n  14,\n  4,\n  30,\n  8,\n  64,\n  8\n)\nprint(digital_word_votes)\n\n[1] 84 14  4 30  8 64  8\n\n\nYou can also create vectors of sequential vectors with the : operator.\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nMore vector creating possibilities\nThere are a lot of functions for creating vectors.\n\nseq(from = 1, to = 5, length = 10)\n\n [1] 1.000000 1.444444 1.888889 2.333333 2.777778 3.222222 3.666667 4.111111\n [9] 4.555556 5.000000\n\n\n\nseq_along(digital_words)\n\n[1] 1 2 3 4 5 6 7\n\n\n\nrep(c(\"a\", \"b\"), times = 3)\n\n[1] \"a\" \"b\" \"a\" \"b\" \"a\" \"b\"\n\n\n\nrep(c(\"a\", \"b\"), each = 3)\n\n[1] \"a\" \"a\" \"a\" \"b\" \"b\" \"b\"\n\n\nVector Arithmetic\nYou can do arithmetic on a whole vector of numbers. digital_word_votes is a vector of how many votes each word got. We can get the sum like so:\n\ntotal_votes &lt;- sum(digital_word_votes)\ntotal_votes\n\n[1] 212\n\n\nThen, we can convert those vote counts to proportions by dividing them by the total.\n\ndigital_word_votes / total_votes\n\n[1] 0.39622642 0.06603774 0.01886792 0.14150943 0.03773585 0.30188679 0.03773585\n\n\nAnd we can convert that to percentages by multiplying by 100.\n\n(digital_word_votes / total_votes) * 100\n\n[1] 39.622642  6.603774  1.886792 14.150943  3.773585 30.188679  3.773585"
  },
  {
    "objectID": "class_notes/2023-01-17_RLang/index.html#indexing",
    "href": "class_notes/2023-01-17_RLang/index.html#indexing",
    "title": "Basics of R Syntax",
    "section": "Indexing",
    "text": "Indexing\nIf you’ve never programmed before, this part will make sense, and if you haven’t programmed before, this part will be confusing.\nIf you have a vector, and you want to get the first value from it, you put square brackets [] after the variable name, and put 1 inside.\n\nprint(digital_words)\n\n[1] \"-dle\"               \"BFFR\"               \"chief twit\"        \n[4] \"chronically online\" \"crypto rug pull\"    \"touch grass\"       \n[7] \"-verse\"            \n\ndigital_words[1]\n\n[1] \"-dle\"\n\n\nIf you want a range of values from a vector, you can give it a vector of numeric indices.\n\ndigital_words[2:5]\n\n[1] \"BFFR\"               \"chief twit\"         \"chronically online\"\n[4] \"crypto rug pull\"   \n\n\nLogical Indexing\nAlso really useful is the ability to do logical indexing. For example, if we wanted to see which digital words got ten or fewer votes, we can do\n\ndigital_word_votes &lt;= 10\n\n[1] FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n\n\nWe can use this sequence of TRUE and FALSE values to get the actual words from the digital_words vector.\n\ndigital_words[digital_word_votes &lt;= 10]\n\n[1] \"chief twit\"      \"crypto rug pull\" \"-verse\""
  },
  {
    "objectID": "class_notes/2023-01-17_RLang/index.html#data-frames",
    "href": "class_notes/2023-01-17_RLang/index.html#data-frames",
    "title": "Basics of R Syntax",
    "section": "Data Frames",
    "text": "Data Frames\nThe most common kind of data structure we’re going to be working with are Data Frames. These are two dimensional structures with rows and columns. The data types within each column all need to be the same.\n\nword_df &lt;- data.frame(\n  type = \"digital\",\n  word = digital_words,\n  votes = digital_word_votes  \n)\nprint(word_df)\n\n     type               word votes\n1 digital               -dle    84\n2 digital               BFFR    14\n3 digital         chief twit     4\n4 digital chronically online    30\n5 digital    crypto rug pull     8\n6 digital        touch grass    64\n7 digital             -verse     8\n\n\nNavigating data frames\nTo navigate data frames, there are a few handy functions. First, in RStudio you can launch a viewer with View()\n\nView(word_df)\n\nKeeping things inside the Quarto notebook, other useful functions are summary(), nrow(), ncol() and colnames().\n\nsummary(word_df)\n\n     type               word               votes      \n Length:7           Length:7           Min.   : 4.00  \n Class :character   Class :character   1st Qu.: 8.00  \n Mode  :character   Mode  :character   Median :14.00  \n                                       Mean   :30.29  \n                                       3rd Qu.:47.00  \n                                       Max.   :84.00  \n\nnrow(word_df)\n\n[1] 7\n\nncol(word_df)\n\n[1] 3\n\ncolnames(word_df)\n\n[1] \"type\"  \"word\"  \"votes\"\n\n\nIndexing Data Frames\nTo get all of the data from a single column of a data frame, we can put $ after the data frame variable name, then the name of the column.\n\nword_df$word\n\n[1] \"-dle\"               \"BFFR\"               \"chief twit\"        \n[4] \"chronically online\" \"crypto rug pull\"    \"touch grass\"       \n[7] \"-verse\"            \n\n\nWe’re going to have more, interesting ways to get specific rows from a data frame later on in the course, but for now if you want to subset just the rows that have 10 or fewer votes, we can use subset.\n\nsubset(word_df, votes &lt;= 10)\n\n     type            word votes\n3 digital      chief twit     4\n5 digital crypto rug pull     8\n7 digital          -verse     8\n\n\n\nPipe Preview\nThe “pipe” (|&gt;) is going to play a big role in our R workflow. What it does is take whatever is on its left hand side and inserts it as the first argument to the function on the left hand side. Here’s a preview.\n\nword_df |&gt; \n  subset(votes &lt;= 10)\n\n     type            word votes\n3 digital      chief twit     4\n5 digital crypto rug pull     8\n7 digital          -verse     8"
  },
  {
    "objectID": "class_notes/2023-01-17_RLang/index.html#packages",
    "href": "class_notes/2023-01-17_RLang/index.html#packages",
    "title": "Basics of R Syntax",
    "section": "Packages",
    "text": "Packages\nPackages get installed once with install.pacakges()\n\n# Only needs to be run once ever, or when updating\ninstall.packages(\"tidyverse\")\n\nBut they need to be loaded every time with library()\n\n# Needs to be run every time\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.0 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nIf you try to load a package that you haven’t installed yet, you’ll get this error:\n\nlibrary(fake_library)\n\nError in library(fake_library): there is no package called 'fake_library'"
  },
  {
    "objectID": "class_notes/2023-01-17_RLang/index.html#footnotes",
    "href": "class_notes/2023-01-17_RLang/index.html#footnotes",
    "title": "Basics of R Syntax",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe reason they aren’t called “lists” is because there’s another kind of data object called a list that has different properties.↩︎"
  },
  {
    "objectID": "class_notes/2023-02-14_linear-models/index.html",
    "href": "class_notes/2023-02-14_linear-models/index.html",
    "title": "Linear Models",
    "section": "",
    "text": "Cast your mind back to algebra days. You might remember the formula for a line being \\(y=mx + b\\). We’re going to be working with that basic concept, except moving around some of the variables and renaming them.\n\\[\ny = \\text{intercept} + (\\text{slope}\\times x)\n\\]\n\\[\ny = b_0 + bx\n\\]"
  },
  {
    "objectID": "class_notes/2023-02-14_linear-models/index.html#the-formula-for-a-line",
    "href": "class_notes/2023-02-14_linear-models/index.html#the-formula-for-a-line",
    "title": "Linear Models",
    "section": "",
    "text": "Cast your mind back to algebra days. You might remember the formula for a line being \\(y=mx + b\\). We’re going to be working with that basic concept, except moving around some of the variables and renaming them.\n\\[\ny = \\text{intercept} + (\\text{slope}\\times x)\n\\]\n\\[\ny = b_0 + bx\n\\]"
  },
  {
    "objectID": "class_notes/2023-02-14_linear-models/index.html#plan",
    "href": "class_notes/2023-02-14_linear-models/index.html#plan",
    "title": "Linear Models",
    "section": "Plan",
    "text": "Plan\n\nReview lines and their geometry\nLines as models\nResiduals\nSum of Squared Residuals\n\\(r^2\\)"
  },
  {
    "objectID": "class_notes/2023-03-02_multivariate/index.html",
    "href": "class_notes/2023-03-02_multivariate/index.html",
    "title": "Starting Multivariate Models",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\nlibrary(marginaleffects)\nword_rt &lt;- read_csv(\"https://raw.githubusercontent.com/bodowinter/applied_statistics_book_data/master/ELP_length_frequency.csv\")\n\nRows: 33075 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Word\ndbl (3): Log10Freq, length, RT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "class_notes/2023-03-02_multivariate/index.html#plots",
    "href": "class_notes/2023-03-02_multivariate/index.html#plots",
    "title": "Starting Multivariate Models",
    "section": "Plots",
    "text": "Plots\nMaking plots of the data before we start modelling:\n\nword_rt |&gt;\n  ggplot(aes(Log10Freq, RT)) +\n    #geom_point()\n    stat_bin_2d()\n\n\n\n\nLooks like a negative effect of word frequency on reaction time\n\nword_rt |&gt;\n  ggplot(aes(length, RT)) +\n    stat_bin_2d()\n\n\n\n\nLooks like a positive effect of word length on reaction time.\n\nword_rt |&gt;\n  ggplot(aes(Log10Freq, length)) +\n    stat_bin_2d()+\n    stat_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nLooks like word frequency and word length are slightly collinear."
  },
  {
    "objectID": "class_notes/2023-03-02_multivariate/index.html#not-ideal-modelling-without-centering-and-scaling",
    "href": "class_notes/2023-03-02_multivariate/index.html#not-ideal-modelling-without-centering-and-scaling",
    "title": "Starting Multivariate Models",
    "section": "Not ideal (modelling without centering and scaling)",
    "text": "Not ideal (modelling without centering and scaling)\nlm fits the model.\n\nmodel_0 &lt;- lm(RT ~ Log10Freq + length, data = word_rt)\n\n\ntidy(model_0)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    748.      2.18      344.        0\n2 Log10Freq      -68.0     0.594    -115.        0\n3 length          19.5     0.238      81.9       0\n\n\nThe (Intercept) value is the predicted value of RT when both Log10Freq and length are 0. Not a likely combination of values.\nGoodness of fit:\n\nglance(model_0)\n\n# A tibble: 1 × 12\n  r.squared adj.r.sq…¹ sigma stati…² p.value    df  logLik    AIC    BIC devia…³\n      &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1     0.487      0.487  89.1  15717.       0     2 -1.95e5 3.91e5 3.91e5  2.62e8\n# … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt;, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance"
  },
  {
    "objectID": "class_notes/2023-03-02_multivariate/index.html#scaling-and-centering",
    "href": "class_notes/2023-03-02_multivariate/index.html#scaling-and-centering",
    "title": "Starting Multivariate Models",
    "section": "Scaling and Centering",
    "text": "Scaling and Centering\nWe’ll center both predictors on their means, and scale by the standard deviation:\n\nword_rt |&gt;\n  mutate(\n    log_freq_z = (Log10Freq - mean(Log10Freq))/sd(Log10Freq),\n    len_z = (length - mean(length))/sd(length)\n  ) -&gt;\n  word_rt_scaled\n\nLet’s actually fit three models,\n\nOne for just frequency\nOne for just length\nOne for frequency and length\n\n\nmodel_f &lt;- lm(RT ~ log_freq_z, data = word_rt_scaled)\nmodel_l &lt;- lm(RT ~ len_z, data = word_rt_scaled)\nmodel_fl &lt;- lm(RT ~ len_z + log_freq_z, data = word_rt_scaled)\n\nWe can examine all of these models goodness of fits together.\n\nlist(\n  freq = model_f,\n  length = model_l,\n  freq_length = model_fl\n) |&gt;\n  map_dfr(glance, .id = \"model\") |&gt; \n  select(model, r.squared, adj.r.squared, AIC, BIC)\n\n# A tibble: 3 × 5\n  model       r.squared adj.r.squared     AIC     BIC\n  &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 freq            0.383         0.383 396956. 396981.\n2 length          0.284         0.284 401905. 401930.\n3 freq_length     0.487         0.487 390856. 390889.\n\n\nThe first thing to notice is that even though we’ve centered and scaled the data, the goodness of fit metrics for the freq_length mode, (r-squared and adjusted r-squared) are identical to the original multivariate model.\nAlso, the goodness of fit always gets better when we add a new predictor, but the adjusted r squared, AIC and BIC try to balance out the complexity of the model and the goodness of fit."
  },
  {
    "objectID": "class_notes/2023-03-02_multivariate/index.html#evaluating-the-model",
    "href": "class_notes/2023-03-02_multivariate/index.html#evaluating-the-model",
    "title": "Starting Multivariate Models",
    "section": "Evaluating the model",
    "text": "Evaluating the model\nWe can merge information from the model onto the original data with augment().\n\naugment(model_fl, word_rt_scaled) |&gt;\n  ggplot(aes(.fitted, .resid))+\n    geom_point()\n\n\n\n\nWe can also look at the predicted values with marginaleffects::predictions()\n\npredictions(model_fl, \n            newdata = datagrid(log_freq_z = c(-1, 0, 1),\n                               len_z = c(-1, 0, 1))) |&gt;\n  tibble()-&gt;\n  predictions_fl\n\n\npredictions_fl |&gt; \n  ggplot(aes(log_freq_z, estimate))+\n    geom_line(aes(color = factor(len_z)))\n\n\n\n\n\npredictions_fl |&gt; \n  ggplot(aes(len_z, estimate))+\n    geom_line(aes(color = factor(log_freq_z)))"
  },
  {
    "objectID": "class_notes/2023-01-12_gh/index.html",
    "href": "class_notes/2023-01-12_gh/index.html",
    "title": "Github Onboarding with RStudio",
    "section": "",
    "text": "Instruction boxes with  should be done on github, and instruction boxes with  should be done in RStudio."
  },
  {
    "objectID": "class_notes/2023-01-12_gh/index.html#step-1-create-a-github-account",
    "href": "class_notes/2023-01-12_gh/index.html#step-1-create-a-github-account",
    "title": "Github Onboarding with RStudio",
    "section": "Step 1: Create a Github Account",
    "text": "Step 1: Create a Github Account\nGo over to Github and create a free account.\n\n\n\n\n\n\n\n\n\n\n\nGo To\n\nhttps://github.com/\n\n\n\n\nAs suggested in Happy Git Chapter 4, it would make sense to register an account username that aligns with your professional identity.\nAfter you’ve created your free account, if you are affiliated with a university, I would also suggest applying for the education benefits here: https://education.github.com/. There are a few nice, but not mandatory, perks."
  },
  {
    "objectID": "class_notes/2023-01-12_gh/index.html#step-2-configure-git-in-rstudio",
    "href": "class_notes/2023-01-12_gh/index.html#step-2-configure-git-in-rstudio",
    "title": "Github Onboarding with RStudio",
    "section": "Step 2: Configure Git in RStudio",
    "text": "Step 2: Configure Git in RStudio\nNow, you need to tell Git a little bit about yourself on the computer/server you’re using RStudio on.\n\n\n\n\n\n\n\n\n\n\n\nGo To\n\nWherever you are using RStudio ( could be Posit Workbench, Posit Cloud, RStudio Server, or RStudio Desktop)\n\nThen Go To\n\nThe Console (a.k.a the R Prompt)\n\n\n\n\n\n\nThe Console in RStudio is here.\n\n\nNext, we need to tell the local version of Git who you are, specifically your username (which should match your Github username) and your email address (which should match the email address you registered for Github with).\n\n\n\n\n\n\n, \n\n\n\nIn the code below, USERNAME should be replaced with your Github username and EMAIL should be replaced with the email you registered your github account with.\n\n\n\nRun this in the R Console:\n\nsystem('git config --global user.name \"USERNAME\"')\nsystem('git config --global user.email \"EMAIL\"')"
  },
  {
    "objectID": "class_notes/2023-01-12_gh/index.html#step-3-configure-rstudio-to-communicate-with-github",
    "href": "class_notes/2023-01-12_gh/index.html#step-3-configure-rstudio-to-communicate-with-github",
    "title": "Github Onboarding with RStudio",
    "section": "Step 3: Configure RStudio to Communicate with Github",
    "text": "Step 3: Configure RStudio to Communicate with Github\nIn order to be able to push commits from RStudio to Github, you’ll need to set up secure communication between wherever you are using RStudio and Github. I’ll walk you through how to do this with SSH credentials. (See also Happy Git with R for personal access tokens via HTTPS).\nRStudio Configuration\n\n\n\n\n\n\n\n\n\n\n\nGo To:\n\nThe Tools menu, then Global Options\n\n\n\n\n\n\n\nThen Go To:\n\nGit/SVN from the left hand side option selector. Its icon is a cardboard box\n\n\n\n\n\n\n\nThen Go To\n\nCreate SSH Key\n\n\n\n\n\n\n\nThen\n\nThe default options should be fine to use. The passphrase here is for the ssh key. It should not be your Github password, or the password for logging into Posit Workbench or Posit Cloud. Once you’re ready, click Create.\n\nThen\n\nAfter creating the SSH key, you should see the option “View Public Key”. Click on it, and copy the text that appears.\n\n\n\n\nThis concludes everything necessary on the RStudio side of things. You should probably keep the session open so that you can come back to re-copy your public key.\nGithub Configuration\nNow, you’ll need to go over to github to add the public key to your profile.\n\n\n\n\n\n\n\n\n\n\n\nGo To\n\nYour Github Profile Settings\n\n\n\n\n\n\n\nThen Go To\n\nSSH and GPG keys from the left side menu\n\n\n\n\n\n\n\nThen\n\nClick on the New SSH key button\n\n\n\n\n\n\n\nThen\n\nGive this key an informative name so you can remember which computer it’s coming from.\n\nThen\n\nPaste the text you copied from RStudio into the Key box and click Add SSH Key."
  },
  {
    "objectID": "class_notes/2023-01-12_gh/index.html#configured",
    "href": "class_notes/2023-01-12_gh/index.html#configured",
    "title": "Github Onboarding with RStudio",
    "section": "Configured",
    "text": "Configured\nNow, wherever you are using RStudio from should be able to push commits to your Github account."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods in Linguistics",
    "section": "",
    "text": "This is the course website for Lin611: Quantitative Methods in Linguistics, taught in the Spring semester of 2023.\n\n\n\n\n\nNotes from in-class will be listed below. Other important information:\n\nPlease read the syllabus\nThe course textbook is Winter, B. (2019). Statistics for linguists: An introduction using R. Routledge. ISBN 978-1-138-05608-4\nKeep an eye on the course Canvas Shell for announcements and assignments.\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nCategorical Predictors\n\n\n\n\n\n\n\n\n\n\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nMultivariate models aren’t just univariate models glued together\n\n\n\n\n\n\n\n\n\n\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nInteractions\n\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nStarting Multivariate Models\n\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nLinear Models in R\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nLinear Models\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nDistributions 2\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nTidy Vowel Normalization\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nTidying and Plotting\n\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nContinuing with the tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nStarting with the tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nDownloading OSF Data\n\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nBasics of R Syntax\n\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nPlan for Onboarding Day\n\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nGithub Onboarding with RStudio\n\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nWelcome Day\n\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\nNo matching items\n\nReuseCC-BY-SA 4.0"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "",
    "text": "Syllabus pdf"
  },
  {
    "objectID": "syllabus.html#key-info",
    "href": "syllabus.html#key-info",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "1 Key info",
    "text": "1 Key info\n\n\n\nWhere and When\n\n\n\nWhere:\nFunkhouser, 307B\n\n\nWhen:\nTuesdays & Thursdays, 12:30 : 13:45\n\n\n\n\n\nInstructor\n\n\n\n\nDr. Josef Fruehwald\n\n\nemail:\njosef.fruehwald@uky.edu\n\n\noffice hours:\nMondays, 14:00 - 15:00\n\n\noffice location:\nPOT 1671"
  },
  {
    "objectID": "syllabus.html#course-at-a-glance",
    "href": "syllabus.html#course-at-a-glance",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "2 Course at a Glance",
    "text": "2 Course at a Glance\n\nWhat you’ll learn:\n\nthe basics of statistical reasoning, linear modelling, data organization & visualization, R\n\nWhat you’ll do:\n\nin-class exercises, a textbook work-through “blog”, a midterm project, a final project.\n\nWhat you’ll need:\n\nthe course textbook, a computing device with a physical keyboard\n\nThe final-est deadline\n\nThursday, May 4\n\n\nAdd to Calendar\n\n\nAttendance Policy\n\nAttendance is crucial for successful completion of the course, but there are no grade penalties.\n\nLate Work Policy\n\n2 day penalty free grace period on all assignments, 5% flat penalty afterwards. See Late Submissions and Re-submissions"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "3 Course Description",
    "text": "3 Course Description\nIn recent decades, there has been a strong “quantitative turn” in linguistics. Quantitative methods, including statistical analysis, have always been fixtures in some subfields, but there are now few areas of linguistic inquiry where they are completely absent. As a graduate course in quantitative methods, the goals of this course are to help you establish baseline statistical reasoning, and to provide practical experience in data (re)organization and statistical model building. We will be focusing our attention on the most common variety of statistical models (linear models and their generalizations) in the most commonly used programming language (R)."
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "4 Learning Outcomes",
    "text": "4 Learning Outcomes\nAfter attending class meetings and completion of the coursework, students should be able to\n\nidentify appropriate quantitative analysis procedures for diverse data sets.\norganize data sets tidily\nre-organize untidy data sets in R\ngenerate exploratory data visualizations\nspecify and fit linear or generalized linear models in R\nreport the meaningful results of a statistical model"
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "5 Course Materials",
    "text": "5 Course Materials\n\nRequired:\nWinter, B. (2019). Statistics for linguists: An introduction using R. Routledge. ISBN 978-1138056091.\n\n\nRecommended:\nWickham, H & G. Grolemund (2022*) R for Data Science https://r4ds.had.co.nz/."
  },
  {
    "objectID": "syllabus.html#course-technology",
    "href": "syllabus.html#course-technology",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "6 Course Technology",
    "text": "6 Course Technology\n\nR/RStudio\nWe’ll be learning how to implement our analyses in the R programming language, specifically using the RStudio IDE. You can install and configure RStudio on your own computer that you bring to class. You will also have access to RStudio Workbench hosted by the College of Arts & Sciences at https://rstudio.as.uky.edu/. You can log in with your LinkBlue credentials.\n\n\nQuarto/Quarto Notebooks\nQuarto is a program built into RStudio that takes source documents written in Markdown and R, and renders them into various output document formats, including html and pdf. This program is included in RStudio, and won’t require additional download or installation.\n\n\nGit/Github\nGit is a “Version Control System” that lets you keep track of changes on software projects. Github is a service that allows online hosting of Git projects. You will need to create a free a Github account for the course.\n\n\nCanvas\nCanvas will be used to make course announcements, and to set & submit assignments."
  },
  {
    "objectID": "syllabus.html#communications",
    "href": "syllabus.html#communications",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "7 Communications",
    "text": "7 Communications\nI will respond to emails in a timely manner during normal working hours, but it may take longer if you email me after 5pm on weekdays, or any time during the weekend."
  },
  {
    "objectID": "syllabus.html#course-schedule",
    "href": "syllabus.html#course-schedule",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "8 Course Schedule",
    "text": "8 Course Schedule\nThe topics and readings listed here are the tentative schedule for the course. We may find, in the room, that some topics will take longer than initially scheduled.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDates\nTopics\nReadings\nNotes\n\n\n\n\n1\nJan 10,12\nOnboarding\nGithub Onboarding with RStudio\nSupplementary Resources\nR4DS: RMarkdown\nQuarto Tutorial: Hello Quarto\nQuarto Tutorial: Computations\nQuarto Tutorial: Authoring\n\n\n2\nJan 17, 19\nIntroduction to R\nWinter, Ch 1\nSupplementary Resources:\nHands-On Programming with R, Chapter 2\n2017 LSA Course: Introduction to R\n\n\n3\nJan 24, 26\nTidyverse and Workflows\nWinter, Ch 2\nSupplementary Resources:\nR4DS, Tidy Data\nR4DS, Data-visualization\n2017 LSA Course: Data Frames\n2017 LSA Course: Split-Apply-Combine\n2017 LSA Course: ggplot2\ntidyr cheat sheet (pdf)\ndplyr cheat sheet (pdf)\nggplot2 cheat sheet (pdf)\n\n\n4\nJan 31, Feb 02\nTidyverse and Workflows (part 2)\nWinter, Ch 2\n’’\n\n\n5\nFeb 07, 09\nDescriptive Statistics, Models, and Distributions\nWinter, Ch3\nSupplementary Resources:\nR4DS, Exploratory Data Analysis\n\n\n6\nFeb 14, 16\nIntro to Linear Models\nWinter Ch4\nSupplementary Resources\nR4DS: Model Basics\n2017 LSA Course: Fitting Linear Models\n\n\n7\nFeb 21, 23\nCorrelations and Transformations\nWinter Ch5\nSupplementary Resources\nR4DS: Model Basics\n2017 LSA Course: Fitting Linear Models\n\n\n8\nFeb 28, Mar 02\nMultiple Regression\nWinter Ch6\nSupplementary Resources\nR4DS: Model Basics\n2017 LSA Course: Fitting Linear Models\n\n\n9\nMar 07, 09\nCategorical Predictors\nWinter Ch7\nMidterm Project Due\nSupplementary Resources\nforcats cheat sheet (pdf)\n\n\nSpring Break\nMar 14, 16\nNo class\nNo Class\n\n\n\n10\nMar 21, 23\nInteractions and Non-Linear Effects\nWinter Ch 8\nSupplementary Material\nSóskuthy (2017)\n\n\n11\nMar 28, 30\nInferential Statistics\nWinter Ch 9,10\nSupplementary Material\nWinter Ch 11\nVisualizing Cohen’s d\nKirby & Sonderegger (2018)\n\n\n12\nApr 04, 06\nGeneralized Linear Models\nWinter Ch 12\nSupplementary Material\nWinter Ch 13\n\n\n13\nApr 11, 13\nMixed Models\nWinter Ch 14\nSupplementary Material\n2017 LSA Course: Mixed Effects Models\n\n\n14\nApr 18, 20\nMixed Models\nWinter Ch 15\nSupplementary Material\n2017 LSA Course: Model Comparison and Bootstrapping\n\n\n15\nApr 25\nReview & Outlook\n\nWinter Ch 16\n\n\nFinals\nMay 04\n\n\nFinal Project Due / The Final-est Deadline"
  },
  {
    "objectID": "syllabus.html#course-evaluation",
    "href": "syllabus.html#course-evaluation",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "9 Course Evaluation",
    "text": "9 Course Evaluation\n\n\n\nGrade Components\n\n\n\nWork-through Blog\n30%\n\n\nExercises\n20%\n\n\nMidterm Project\n20%\n\n\nFinal Project\n20%\n\n\nEngagement\n10%\n\n\n\n\n\nGrading Scale\n\n\n\nA\n&gt;= 90\n\n\nB\n80 to 89\n\n\nC\n70 to 79\n\n\nD\n60 to 69\n\n\nE\n&lt;= 59\n\n\n\n\n\n\n\nAssignment Submission\nAssignments will be set on canvas, and you will submit notification of the assignment’s completion through canvas. The content of the assignment itself may be contained elsewhere (e.g. the A&S RStudio server, or a Github repository).\n\n\nWork-through Blog\nAs we work through Winter (2019) chapter by chapter, you will need to update a Quarto blog in which you, at the very least, run every code chunk from the chapter. A template Quarto blog repository is already available on Github. These blog posts will be due at the end of the week in which we finish covering each chapter.\n\n\nExercises\nIn addition to running the code included in each chapter, there will also be occasional R programming exercises.\n\n\nMidterm Project\nThere will be a midterm project to analyze a sample data set utilizing the methods covered in the course up to that point, and to report on your analysis.\n\n\nFinal Project\nThere will also be a final project in the same format as the midterm project, but to extend your analysis tools to the fuller suite of methods covered in the course.\n\n\nEngagement\nInspired by Kirby Conrod’s approach to Participation Grades\n\nThis portion of the grade is a way for me to give you credit for informal/unstructured collaborative work that you do. Participation and collaboration are strong predictors of success and learning retention, so please make an effort to find a way that works well for you to participate and engage with your colleagues.\n\nA well known process for solving programming problems is “Rubber Duck Debugging.” It works by describing how each step of a program is supposed to work to another person or, as the name suggests, a rubber duck. Often the solution to the problem or the typo causing the bug jumps out at you during the process. Having a study buddy or study group could be really helpful if only for this purpose."
  },
  {
    "objectID": "syllabus.html#late-submissions-and-re-submissions",
    "href": "syllabus.html#late-submissions-and-re-submissions",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "10 Late Submissions and Re-submissions",
    "text": "10 Late Submissions and Re-submissions\nEvery graded piece of work will have a due date. After a 2 day grace period, there will be a single, flat 5% deduction from late work, whenever it is submitted between the due date and the The Final-est Deadline\n\nMidterm Grades\nAdd to Calendar\n\nI will submit midterm grades on March 13, 2023, at the end of the midterm grading window. Any unsubmitted assignments that were due before March 13 will be given a grade of 0, BUT you can still submit those assignments after March 13 for their inclusion in the final grade.\n\n\nThe Final-est Deadline\nAdd to Calendar\n\nThe final-est deadline by which to submit any material to be graded is May 4, 2023. I have to set this hard deadline in order to have enough time to conclude final grading in time for the university’s final grade submission deadline."
  },
  {
    "objectID": "syllabus.html#group-work-and-code-sources",
    "href": "syllabus.html#group-work-and-code-sources",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "11 Group Work and Code Sources",
    "text": "11 Group Work and Code Sources\nIt is acceptable to collaborate and confer with other students in the course. Any collaboration should be indicated in the assignment submission. You may also refer to code sources from elsewhere on the internet, as long as you also document the source, and explain what the code does. You might not receive credit for code which has been copied wholesale from another online source or from another student without credit or documentation.\n\nLarge Language Model (a.k.a. AI) Generated Code\nThere are a number of services that will generate code based on natural language queries. Some words of warning:\n\nFluent BS\nLarge Language Models have been found to generate code that looks superficially correct, but often does not actually run properly, or do what the human asker wanted. Being able to successfully identify where or why code does not work correctly is not always straight forward. This issue led the Q&A site StackOverflow to ban submissions generated by LLMs, stating\n\n[…] because GPT is good enough to convince users of the site that the answer holds merit, signals the community typically use to determine the legitimacy of their peers’ contributions frequently fail to detect severe issues with GPT-generated answers.\n\n\n\nExplain what the code does\nAs stated above, you should provide credit to any external sources you turned to for code help, and explain what the resulting code does."
  },
  {
    "objectID": "syllabus.html#attendance-and-engagement",
    "href": "syllabus.html#attendance-and-engagement",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "12 Attendance and Engagement",
    "text": "12 Attendance and Engagement\nYou are expected to attend all scheduled course meetings. It would be helpful, but not necessary, if you let me know in advance if you are going to miss any lectures.\nIf you feel sick in any way, including but not limited to the well-known symptoms of COVID-19 (loss of taste or smell, a new and persistent cough, high fever, etc), do not come to class. There are other mechanisms for demonstrating engagement than attending lectures.\nI will also expect all of us in the course to treat each other with respect and civility in all aspects of the course, including\n\nIn the audio of a Zoom meeting\nIn the text chat of a Zoom meeting\nOn any course discussion boards or other forums."
  },
  {
    "objectID": "syllabus.html#academic-conduct",
    "href": "syllabus.html#academic-conduct",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "13 Academic Conduct",
    "text": "13 Academic Conduct\nUK Senate rules on academic offences: https://www.uky.edu/universitysenate/ao\nAppropriating someone else’s work and portraying it as your own is cheating. Collaborating with someone and portraying that work as solely your own is cheating. Obtaining answers to homework assignments or exams from previous semesters is cheating. Using an internet search engine to look up a question and reporting that answer as your own is cheating. Falsifying data or experimental results is cheating. If you are unsure about whether a specific action is cheating, you may check with me.\nThe minimum penalty for a first offense is a zero on the assignment on which the offense occurred. If the offense is considered severe or if the student has other academic offenses on their record, more serious penalties, up to suspension from the University may be imposed.\nWhen students submit work purporting to be their own, but which in any way borrows ideas, organization, wording or anything else from another source without appropriate acknowledgement of the fact, the students are guilty of plagiarism. Plagiarism includes reproducing someone else’s work, whether it be a published article, chapter of a book, a paper from a friend or some file, or something similar to this. Plagiarism also includes the practice of employing or allowing another person to alter or revise the work which a student submits as their own, whoever that other person may be.\nStudents may discuss assignments among themselves or with an instructor or tutor, but when the actual work is done, it must be done by the student, and the student alone. When a student’s assignment involves research in outside sources of information, the student must carefully acknowledge exactly what, where and how they employed them. If the words of someone else are used, the student must put quotation marks around the passage in question and add an appropriate indication of its origin. Making simple changes while leaving the organization, content and phraseology intact is plagiaristic. However, nothing in these Rules shall apply to those ideas which are so generally and freely circulated as to be a part of the public domain (University Senate Rules Section 6.3.1)."
  },
  {
    "objectID": "syllabus.html#university-academic-policy-statements",
    "href": "syllabus.html#university-academic-policy-statements",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "14 University Academic Policy Statements",
    "text": "14 University Academic Policy Statements\nLink to University Senate Academic Policy Statements https://www.uky.edu/universitysenate/acadpolicy\n\nExcused Absences and Acceptable Excuses\n\nExcused Absences: Senate Rules 5.2.5.2.1 defines the following as acceptable reasons for excused absences: (a) significant illness, (b) death of a family member, (c) trips for members of student organizations sponsored by an educational unit, trips for University classes, and trips for participation in intercollegiate athletic events, (d) major religious holidays, (e) interviews for graduate/professional school or full-time employment post-graduation, and (f) other circumstances found to fit “reasonable cause for nonattendance” by the instructor of record. Students should notify the professor of absences prior to class when possible.\nIf a course syllabus requires specific interactions (e.g., with the instructor or other students), in situations where a student’s total EXCUSED absences exceed 1/5 (or 20%) of the required interactions for the course, the student shall have the right to request and receive a “W,” or the Instructor of Record may award an “I” for the course if the student declines a “W.” (Senate Rules 5.2.5.2.3.1)\n\n\n\nReligious Observances\n\nReligious Observances: Students anticipating an absence for a major religious holiday are responsible for notifying the instructor in writing of anticipated absences due to their observance of such holidays. Senate Rules 5.2.5.2.1(4) requires faculty to include any notification requirements within the syllabus. If no requirement is specified, two weeks prior to the absence is reasonable and should not be given any later. Information regarding major religious holidays may be obtained through the Ombud’s websiteor calling 859-257-3737.\n\n\n\nVerification of Absences\n\nVerification of Absences:Students may be asked to verify their absences in order for them to be considered excused. Senate Rule 5.2.5.2.1 states that faculty have the right to request appropriate verification when students claim an excused absence due to: significant illness; death in the household, trips for classes, trips sponsored by an educational unit and trips for participation related to intercollegiate athletic events; and interviews for full-time job opportunities after graduation and interviews for graduate and professional school. (Appropriate notification of absences due to University-related trips is required prior to the absence when feasible and in no case more than one week after the absence.)\n\n\n\nMake-Up Work\n\nMake-Up Work: Students missing any graded work due to an excused absence are responsible: for informing the Instructor of Record about their excused absence within one week following the period of the excused absence (except where prior notification is required); and for making up the missed work. The instructor must give the student an opportunity to make up the work and/or the exams missed due to the excused absence, and shall do so, if feasible, during the semester in which the absence occurred. The instructor shall provide the student with an opportunity to make up the graded work and may not simply calculate the student’s grade on the basis of the other course requirements, unless the student agrees in writing. According to SR 5.2.5.2.2, if a student adds a class after the first day of classes and misses graded work, the instructor must provide the student with an opportunity to make up any graded work.\n\n\n\nExcused Absences for Military Duties\n\nExcused Absences for Military Duties: If a student is required to be absent for one-fifth or less of the required course interactions (e.g., class meetings) due to military duties, the following procedure (per SR 5.2.5.2.3.2) shall apply:\n\nOnce a student is aware of a call to duty, the student shall provide a copy of the military orders to the Director of the Veterans Resource Center. The student shall also provide the Director with a list of his/her courses and instructors.\nThe Director will verify the orders with the appropriate military authority, and on behalf of the military student, notify each Instructor of Record via Department Letterhead as to the known extent of the absence.\nThe Instructor of Record shall not penalize the student’s absence in any way and shall provide accommodations and timeframes so that the student can make up missed assignments, quizzes, and tests in a mutually agreed upon manner.\n\n\n\n\nUnexcused Absences\n\nUnexcused Absences: If an attendance/interaction policy is not stated in the course syllabus or the policy does not include a penalty to the student, the instructor cannot penalize a student for any unexcused absences. (SR 5.2.5.2.3.3)\n\n\n\nPrep Week and Reading Days\n\nPrep Week and Reading Days: Per Senate Rules 5.2.5.6, the last week of instruction of a regular semester is termed “Prep Week.” This phrase also refers to the last three days of instruction of the summer session and winter intersession. The Prep Week rule applies to ALL courses taught in the fall semester, spring semester, and summer session, including those taught by distance learning or in a format that has been compressed into less than one semester or session. This rule does not apply to courses in professional programs in colleges that have University Senate approval to have their own calendar.\nMake-up exams and quizzes are allowed during Prep Week. In cases of “Take Home” final examinations, students shall not be required to return the completed examination before the regularly scheduled examination period for that course. No written examinations, including final examinations, may be scheduled during the Prep Week. No quizzes may be given during Prep Week. No project/lab practicals/paper/presentation deadlines or oral/listening examinations may fall during the Prep Week unless it was scheduled in the syllabus AND the course has no final examination (or assignment that acts as a final examination) scheduled during finals week. (A course with a lab component may schedule the lab practical of the course during Prep Week if the lab portion does not also require a Final Examination during finals week.) Class participation and attendance grades are permitted during Prep Week. The Senate Rules permit continuing into Prep Week regularly assigned graded homework that was announced in the class syllabus.\nFor fall and spring semester, the Thursday and Friday of Prep Week are study days (i.e. “Reading Days”). There cannot be any required “interactions” on a Reading Day. “Interactions” include participation in an in-class or online discussion, attendance at a guest lecture, or uploading an assignment. See Senate Rules 9.1 for a more complete description of required interactions.\n\n\n\nAccommodations Due to Disability\n\nAccommodations Due to Disability: In accordance with federal law, if you have a documented disability that requires academic accommodations, please inform your instructor as soon as possible during scheduled office hours. In order to receive accommodations in a course, you must provide your instructor with a Letter of Accommodation from the Disability Resource Center (DRC). The DRC coordinates campus disability services available to students with disabilities. It is located on the corner of Rose Street and Huguelet Drive in the Multidisciplinary Science Building, Suite 407. You can reach them via phone at (859) 257-2754, via email (drc@uky.edu) or visit their website (uky.edu/DisabilityResourceCenter). DRC accommodations are not retroactive and should therefore be established with the DRC as early in the semester as is feasible.\n\n\n\nNon-Discrimination Statement and Title IX Information\n\nNon-discrimination and Title IX policy: In accordance with federal law, UK is committed to providing a safe learning, living, and working environment for all members of the University community. The University maintains a comprehensive program which protects all members from discrimination, harassment, and sexual misconduct. For complete information about UK’s prohibition on discrimination and harassment on aspects such as race, color, ethnic origin, national origin, creed, religion, political belief, sex, and sexual orientation, please see the electronic version of UK’s Administrative Regulation 6:1 (“Policy on Discrimination and Harassment”) (https://www.uky.edu/regs/ar6-1). In accordance with Title IX of the Education Amendments of 1972, the University prohibits discrimination and harassment on the basis of sex in academics, employment, and all of its programs and activities. Sexual misconduct is a form of sexual harassment in which one act is severe enough to create a hostile environment based on sex and is prohibited between members of the University community and shall not be tolerated. For more details, please see the electronic version of Administrative Regulations 6:2 (“Policy and Procedures for Addressing and Resolving Allegations of Sexual Harassment Under Title IX and Other Forms of Sexual Misconduct”) (https://www.uky.edu/regs/sites/www.uky.edu.regs/files/files/ar/ar_6.2-in...). Complaints regarding violations of University policies on discrimination, harassment, and sexual misconduct are handled by the Office of Institutional Equity and Equal Opportunity (Institutional Equity), which is located in 13 Main Building and can be reached by phone at (859) 257-8927. You can also visit Institutional Equity’s website (https://www.uky.edu/eeo).\nFaculty members are obligated to forward any report made by a student related to discrimination, harassment, and sexual misconduct to the Office of Institutional Equity. Students can confidentially report alleged incidences through the Violence Intervention and Prevention Center (https://www.uky.edu/vipcenter), Counseling Center (https://www.uky.edu/counselingcenter), or University Health Service (https://ukhealthcare.uky.edu/university-health-service/student-health).\nReports of discrimination, harassment, or sexual misconduct may be made via the Institutional Equity’s website (https://www.uky.edu/eeo); at that site, click on “Make a Report” on the left-hand side of the page.\n\n\n\nRegular and Substantive Interaction\n\nRegular and Substantive Interaction: All credit-bearing courses must support regular and substantive interaction (RSI) between the students and the instructor, regardless of the course’s delivery mode (e.g., in-person, hybrid, or online). Courses satisfy this requirement when course participants meet regularly as prescribed in SR 10.6, and the Instructor of Record substantively interacts with students in at least two of the following ways: provides direct instruction; assesses students’ learning; provides information or responds to students’ questions; and facilitates student discussions. Some exceptions allowed as per SACSCOC. For further information about the RSI requirement, see the Compliance Resources link on the Teaching, Learning and Academic Innovation Compliance page."
  },
  {
    "objectID": "syllabus.html#diversity-equity-and-inclusion",
    "href": "syllabus.html#diversity-equity-and-inclusion",
    "title": "Lin611-001: Quantitative Methods in Linguistics",
    "section": "15 Diversity, Equity and Inclusion",
    "text": "15 Diversity, Equity and Inclusion\nThe University of Kentucky is committed to our core values of diversity and inclusion, mutual respect and human dignity, and a sense of community (Governing Regulations XIV). We acknowledge and respect the seen and unseen diverse identities and experiences of all members of the university community (https://www.uky.edu/regs/gr14). These identities include but are not limited to those based on race, ethnicity, gender identity and expressions, ideas and perspectives, religious and cultural beliefs, sexual orientation, national origin, age, ability, and socioeconomic status. We are committed to equity and justice and providing a learning and engaging community in which every member is engaged, heard, and valued.\nWe strive to rectify and change behavior that is inconsistent with our principles and commitment to diversity, equity, and inclusion. If students encounter such behavior in a course, they are encouraged to speak with the instructor of record and/or the Office of Institutional Equity and Equal Opportunity. Students may also contact a faculty member within the department, program director, the director of undergraduate or graduate studies, the department chair, any college administrator, or the dean. All of these individuals are mandatory reporters under University policies.\nhttps://www.uky.edu/universitysenate/syllabus-dei"
  },
  {
    "objectID": "class_notes.html",
    "href": "class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "Categorical Predictors\n\n\n\n\n\n\n\n\n\n\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nMultivariate models aren’t just univariate models glued together\n\n\n\n\n\n\n\n\n\n\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nInteractions\n\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nStarting Multivariate Models\n\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nLinear Models in R\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nLinear Models\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nDistributions 2\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nTidy Vowel Normalization\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nTidying and Plotting\n\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nContinuing with the tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nStarting with the tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nDownloading OSF Data\n\n\n\n\n\n\n\nosf\n\n\ndata download\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nBasics of R Syntax\n\n\n\n\n\n\n\nR basics\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nPlan for Onboarding Day\n\n\n\n\n\n\n\nBlock 1\n\n\nOnboarding\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nGithub Onboarding with RStudio\n\n\n\n\n\n\n\nOnboarding\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\n  \n\n\n\n\nWelcome Day\n\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\nJosef Fruehwald\n\n\n\n\n\n\nNo matching items\n\nReuseCC-BY-SA 4.0"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The topics and readings listed here are the tentative schedule for the course. We may find, in the room, that some topics will take longer than initially scheduled.\n\n\n\nWeek\nDates\nTopics\nReadings\nSupplementary Resources\nNotes\n\n\n\n\n1\nJan 10,12\nOnboarding\nGithub Onboarding with RStudio\nR4DS: RMarkdown\nQuarto Tutorial: Hello Quarto\nQuarto Tutorial: Computations\nQuarto Tutorial: Authoring\n\n\n\n2\nJan 17, 19\nIntroduction to R\nWinter, Ch 1\nHands-On Programming with R, Chapter 2\n2017 LSA Course: Introduction to R\nChapter 1 blog due Friday\n\n\n3\nJan 24, 26\nTidyverse and Workflows\nWinter, Ch 2\nR4DS, Tidy Data\nR4DS, Data-visualization\n2017 LSA Course: Data Frames\n2017 LSA Course: Split-Apply-Combine\n2017 LSA Course: ggplot2\ntidyr cheat sheet (pdf)\ndplyr cheat sheet (pdf)\nggplot2 cheat sheet (pdf)\n\n\n\n4\nJan 31, Feb 02\nTidyverse and Workflows (part 2)\nWinter, Ch 2\n\nChapter 2 blog due Friday\n\n\n5\nFeb 07, 09\nDescriptive Statistics, Models, and Distributions\nWinter, Ch3\nR4DS, Exploratory Data Analysis\nChapter 3 blog due Friday\n\n\n6\nFeb 14, 16\nIntro to Linear Models\nWinter Ch4\nR4DS: Model Basics\n2017 LSA Course: Fitting Linear Models\nChapter 4 blog due Friday\n\n\n7\nFeb 21, 23\nCorrelations and Transformations\nWinter Ch5\nR4DS: Model Basics\n2017 LSA Course: Fitting Linear Models\nChapter 5 blog due Friday\n\n\n8\nFeb 28, Mar 02\nMultiple Regression\nWinter Ch6\nR4DS: Model Basics\n2017 LSA Course: Fitting Linear Models\nChapter 6 blog due Friday\n\n\n9\nMar 07, 09\nCategorical Predictors\nWinter Ch7\nforcats cheat sheet (pdf)\nChapter 7 blog due Friday\nMidterm Project Due\n\n\nSpring Break\nMar 14, 16\nNo class\nNo Class\n\n\n\n\n10\nMar 21, 23\nInteractions and Non-Linear Effects\nWinter Ch 8\nSóskuthy (2017)\nChapter 8 blog due Friday\n\n\n11\nMar 28, 30\nInferential Statistics\nWinter Ch 9,10\nWinter Ch 11\nVisualizing Cohen’s d\nKirby & Sonderegger (2018)\nChapter 9, 10 blog due Friday\n\n\n12\nApr 04, 06\nMixed Models\nWinter Ch 14\n2017 LSA Course: Mixed Effects Models\nChapter 14 blog due Friday\n\n\n13\nApr 11, 13\nMixed Models\nWinter Ch 15\n2017 LSA Course: Model Comparison and Bootstrapping\nChapter 15 blog due Friday\n\n\n14\nApr 18, 20\nGeneralized Linear Models\nWinter Ch 12\nWinter Ch 13\nChapter 12 blog due Friday\n\n\n15\nApr 25\nReview & Outlook\n\nWinter Ch 16\n\n\n\nFinals\nMay 04\n\n\n\nFinal Project Due\n\n\n\n\n\n\nReuseCC-BY-SA 4.0"
  }
]